% Encoding: UTF-8

@Article{doi:10.1121/1.4950011,
  author  = {Matthew Edwards},
  title   = {The affect of audio enhancement on vocal timbre},
  journal = {The Journal of the Acoustical Society of America},
  year    = {2016},
  volume  = {139},
  number  = {4},
  pages   = {2034-2034},
  doi     = {10.1121/1.4950011},
  eprint  = {http://dx.doi.org/10.1121/1.4950011},
  url     = { 
        http://dx.doi.org/10.1121/1.4950011
    
},
}

@Article{5410057,
  author   = {H. Fujihara and M. Goto and T. Kitahara and H. G. Okuno},
  title    = {A Modeling of Singing Voice Robust to Accompaniment Sounds and Its Application to Singer Identification and Vocal-Timbre-Similarity-Based Music Information Retrieval},
  journal  = {IEEE Transactions on Audio, Speech, and Language Processing},
  year     = {2010},
  volume   = {18},
  number   = {3},
  pages    = {638-648},
  month    = {March},
  issn     = {1558-7916},
  abstract = {This paper describes a method of modeling the characteristics of a singing voice from polyphonic musical audio signals including sounds of various musical instruments. Because singing voices play an important role in musical pieces with vocals, such representation is useful for music information retrieval systems. The main problem in modeling the characteristics of a singing voice is the negative influences caused by accompaniment sounds. To solve this problem, we developed two methods, accompaniment sound reduction and reliable frame selection . The former makes it possible to calculate feature vectors that represent a spectral envelope of a singing voice after reducing accompaniment sounds. It first extracts the harmonic components of the predominant melody from sound mixtures and then resynthesizes the melody by using a sinusoidal model driven by these components. The latter method then estimates the reliability of frame of the obtained melody (i.e., the influence of accompaniment sound) by using two Gaussian mixture models (GMMs) for vocal and nonvocal frames to select the reliable vocal portions of musical pieces. Finally, each song is represented by its GMM consisting of the reliable frames. This new representation of the singing voice is demonstrated to improve the performance of an automatic singer identification system and to achieve an MIR system based on vocal timbre similarity.},
  doi      = {10.1109/TASL.2010.2041386},
  file     = {:5410057 - A Modeling of Singing Voice Robust to Accompaniment Sounds and Its Application to Singer Identification and Vocal-Timbre-Similarity-Based Music Information Retrieval.pdf:PDF},
  keywords = {Gaussian processes;audio signal processing;feature extraction;information retrieval;Gaussian mixture models;accompaniment sound reduction;frame selection reliability;musical instruments;polyphonic musical audio signals;predominant melody;singer identification;vocal-timbre-similarity-based music information retrieval;Feature extraction;Helium;Instruments;Multiple signal classification;Music information retrieval;Robustness;Signal processing;Timbre;Music information retrieval (MIR);singer identification;singing voice;vocal;vocal timbre similarity},
}

@InProceedings{6854595,
  author    = {T. Nakano and K. Yoshii and M. Goto},
  title     = {Vocal timbre analysis using latent Dirichlet allocation and cross-gender vocal timbre similarity},
  booktitle = {2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2014},
  pages     = {5202-5206},
  month     = {May},
  abstract  = {This paper presents a vocal timbre analysis method based on topic modeling using latent Dirichlet allocation (LDA). Although many works have focused on analyzing characteristics of singing voices, none have dealt with “latent” characteristics (topics) of vocal timbre, which are shared by multiple singing voices. In the work described in this paper, we first automatically extracted vocal timbre features from polyphonic musical audio signals including vocal sounds. The extracted features were used as observed data, and mixing weights of multiple topics were estimated by LDA. Finally, the semantics of each topic were visualized by using a word-cloud-based approach. Experimental results for a singer identification task using 36 songs sung by 12 singers showed that our method achieved a mean reciprocal rank of 0.86. We also proposed a method for estimating cross-gender vocal timbre similarity by generating pitch-shifted (frequency-warped) signals of every singing voice. Experimental results for a cross-gender singer retrieval task showed that our method discovered interesting similar pitch-shifted singers.},
  doi       = {10.1109/ICASSP.2014.6854595},
  file      = {:6854595 - Vocal timbre analysis using latent Dirichlet allocation and cross-gender vocal timbre similarity.pdf:PDF},
  issn      = {1520-6149},
  keywords  = {audio signals;feature extraction;speech processing;LDA;automatically extracted vocal timbre features;cross-gender singer retrieval task;cross-gender vocal timbre similarity;frequency-warped signals;latent Dirichlet allocation;latent characteristics;mean reciprocal rank;mixing weights;multiple singing voices;observed data;pitch-shifted signals;polyphonic musical audio signals;singer identification task;vocal sounds;vocal timbre analysis;word cloud;Estimation;Feature extraction;Resource management;Timbre;Vectors;Visualization;cross-gender similarity;latent Dirichlet allocation;music information retrieval;vocal timbre;word cloud},
}

@Article{6739213,
  author   = {J. Salamon and E. Gomez and D. P. W. Ellis and G. Richard},
  title    = {Melody Extraction from Polyphonic Music Signals: Approaches, applications, and challenges},
  journal  = {IEEE Signal Processing Magazine},
  year     = {2014},
  volume   = {31},
  number   = {2},
  pages    = {118-134},
  month    = {March},
  issn     = {1053-5888},
  abstract = {Melody extraction algorithms aim to produce a sequence of frequency values corresponding to the pitch of the dominant melody from a musical recording. Over the past decade, melody extraction has emerged as an active research topic, comprising a large variety of proposed algorithms spanning a wide range of techniques. This article provides an overview of these techniques, the applications for which melody extraction is useful, and the challenges that remain. We start with a discussion of ?melody? from both musical and signal processing perspectives and provide a case study that interprets the output of a melody extraction algorithm for specific excerpts. We then provide a comprehensive comparative analysis of melody extraction algorithms based on the results of an international evaluation campaign. We discuss issues of algorithm design, evaluation, and applications that build upon melody extraction. Finally, we discuss some of the remaining challenges in melody extraction research in terms of algorithmic performance, development, and evaluation methodology.},
  doi      = {10.1109/MSP.2013.2271648},
  file     = {:6739213 - Melody Extraction from Polyphonic Music Signals_ Approaches, applications, and challenges.pdf:PDF},
  keywords = {audio signal processing;information retrieval;music;algorithm design;algorithm evaluation;audio signal processing;dominant melody pitch;frequency value sequence;international evaluation campaign;melody extraction algorithms;music information retrieval;musical processing perspectives;musical recording;polyphonic music signals;Data mining;Harmonic analysis;Instruments;Multiple signal classification;Music;Signal processing algorithms;Time-frequency analysis},
}

@InProceedings{Wu:2014:MER:2647868.2654904,
  author    = {Wu, Bin and Zhong, Erheng and Horner, Andrew and Yang, Qiang},
  title     = {Music Emotion Recognition by Multi-label Multi-layer Multi-instance Multi-view Learning},
  booktitle = {Proceedings of the 22Nd ACM International Conference on Multimedia},
  year      = {2014},
  series    = {MM '14},
  pages     = {117--126},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {2654904},
  doi       = {10.1145/2647868.2654904},
  file      = {:Music Emotion Recognition by Multi-label Multi-layer Multi-instance Multi-view Learning.pdf:PDF},
  isbn      = {978-1-4503-3063-3},
  keywords  = {multi-label multi-layer multi-instance multi-view learning, music emotion recognition},
  location  = {Orlando, Florida, USA},
  numpages  = {10},
  url       = {http://doi.acm.org/10.1145/2647868.2654904},
}

@Article{6792454,
  author  = {B. L. Sturm},
  title   = {Alexander Lerch: An Introduction to Audio Content Analysis: Applications in Signal Processing and Music Informatics},
  journal = {Computer Music Journal},
  year    = {2013},
  volume  = {37},
  number  = {4},
  pages   = {90-91},
  month   = {Dec},
  issn    = {0148-9267},
  doi     = {10.1162/COMJ_r_00208},
}

@InBook{6331125,
  pages     = {272-},
  title     = {Musical Genre, Similarity, and Mood},
  publisher = {Wiley-IEEE Press},
  year      = {2012},
  author    = {Alexander Lerch},
  isbn      = {9781118393550},
  abstract  = {
This chapter contains sections titled:

Musical Genre Classification

Related Research Fields

},
  booktitle = {An Introduction to Audio Content Analysis:Applications in Signal Processing and Music Informatics},
  doi       = {10.1002/9781118393550.ch8},
  keywords  = {Feature extraction;Instruments;Mood;Music;Taxonomy},
  url       = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6331125},
}

@InBook{6331056,
  pages     = {272-},
  title     = {Music Performance Analysis},
  publisher = {Wiley-IEEE Press},
  year      = {2012},
  author    = {Alexander Lerch},
  isbn      = {9781118393550},
  abstract  = {
This chapter contains sections titled:

Musical Communication

Music Performance Analysis

},
  booktitle = {An Introduction to Audio Content Analysis:Applications in Signal Processing and Music Informatics},
  doi       = {10.1002/9781118393550.ch10},
  keywords  = {Instruments;Multiple signal classification;Music;Performance analysis;Production;Psychology},
  url       = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6331056},
}

@Article{5709966,
  author   = {M. Muller and D. P. W. Ellis and A. Klapuri and G. Richard},
  title    = {Signal Processing for Music Analysis},
  journal  = {IEEE Journal of Selected Topics in Signal Processing},
  year     = {2011},
  volume   = {5},
  number   = {6},
  pages    = {1088-1110},
  month    = {Oct},
  issn     = {1932-4553},
  abstract = {Music signal processing may appear to be the junior relation of the large and mature field of speech signal processing, not least because many techniques and representations originally developed for speech have been applied to music, often with good results. However, music signals possess specific acoustic and structural characteristics that distinguish them from spoken language or other nonmusical signals. This paper provides an overview of some signal analysis techniques that specifically address musical dimensions such as melody, harmony, rhythm, and timbre. We will examine how particular characteristics of music signals impact and determine these techniques, and we highlight a number of novel music analysis and retrieval tasks that such processing makes possible. Our goal is to demonstrate that, to be successful, music audio signal processing techniques must be informed by a deep and thorough insight into the nature of music itself.},
  doi      = {10.1109/JSTSP.2011.2112333},
  file     = {:5709966 - Signal Processing for Music Analysis.pdf:PDF},
  keywords = {audio signal processing;music;source separation;harmony;melody;music analysis;music audio signal processing techniques;rhythm;timbre;Harmonic analysis;Instruments;Multiple signal classification;Music;Spectrogram;Time frequency analysis;Beat;digital signal processing;harmony;melody;music analysis;music information retrieval;music signals;pitch;rhythm;source separation;timbre;voice separation},
}

@Article{AMIR2005252,
  author        = {Ofer Amir and Noam Amir and Orit Michaeli},
  title         = {Evaluating the Influence of Warmup on Singing Voice Quality Using Acoustic Measures},
  journal       = {Journal of Voice},
  year          = {2005},
  volume        = {19},
  number        = {2},
  pages         = {252 - 260},
  issn          = {0892-1997},
  __markedentry = {[xiaoc:1]},
  abstract      = {Vocal warmup is generally accepted as vital for singing performance. However, only a limited number of studies have evaluated this effect quantitatively. In this study, we evaluated the effect of vocal warmup on voice production, among young female singers, using a set of acoustic parameters. Warmup reduced frequency-perturbation (p < 0.001) and amplitude-perturbation values (p < 0.05). In addition, warmup increased singer's formant amplitude (p < 0.05) and improved noise-to-harmonic ratio (p < 0.05). Tone-matching accuracy, however, was not affected by warmup. The effect of vocal warmup on frequency-perturbation parameters was more evident among mezzo-soprano singers than it was among soprano singers. It was also more evident in the low pitch-range than in the higher pitch-ranges (p < 0.05). The results of this study provide valid support for the advantageous effect of vocal warmup on voice quality and present acoustic analysis as a valuable and sensitive tool for quantifying this effect.},
  doi           = {https://doi.org/10.1016/j.jvoice.2004.02.008},
  file          = {:Evaluating the Influence of Warmup on Singing Voice Quality Using Acoustic Measures.pdf:PDF},
  keywords      = {Warmup, Acoustic analysis, Singing voice, Voice quality},
  url           = {http://www.sciencedirect.com/science/article/pii/S0892199704000402},
}

@Article{GISH2012e1,
  author   = {Allison Gish and Melda Kunduk and Loraine Sims and Andrew J. McWhorter},
  title    = {Vocal Warm-Up Practices and Perceptions in Vocalists: A Pilot Survey},
  journal  = {Journal of Voice},
  year     = {2012},
  volume   = {26},
  number   = {1},
  pages    = {e1 - e10},
  issn     = {0892-1997},
  abstract = {Summary Investigated in a pilot study the type, duration, and frequency of vocal warm-up regimens in the singing community using a survey. One hundred seventeen participants completed an online survey. Participants included voice students from undergraduate, masters, and doctoral music programs and professional singers. Fifty-four percent of participants reported always using vocal warm-up before singing. Twenty-two percent of the participants used vocal cool down. The most preferred warm-up duration was of 5–10 minutes in duration. Despite using vocal warm-up, 26% of the participants reported experiencing voice problems. Females tended to use vocal warm-up more frequently than males. Females also tended to use longer warm-up sessions than males. Education of the participants did not appear to have any noticeable effect on the vocal warm-up practices. The most commonly used singing warm-up exercises were ascending/descending five-note scales, ascending/descending octave scales, legato arpeggios, and glissandi.},
  doi      = {https://doi.org/10.1016/j.jvoice.2010.10.005},
  keywords = {Vocal warm-up, Vocal cool-down, Vocal injury},
  url      = {http://www.sciencedirect.com/science/article/pii/S089219971000175X},
}

@Article{bartholomew1934physical,
  author    = {Bartholomew, Wilmer T},
  title     = {A Physical Definition of “Good Voice-Quality” in the Male Voice},
  journal   = {the Journal of the Acoustical Society of America},
  year      = {1934},
  volume    = {6},
  number    = {1},
  pages     = {25--33},
  file      = {:A Physical Definition of “Good Voice‐Quality” in the Male Voice.pdf:PDF},
  publisher = {ASA},
}

@Article{MOORCROFT2013258.e1,
  author   = {Lynda Moorcroft and Dianna T. Kenny},
  title    = {Singer and Listener Perception of Vocal Warm-Up},
  journal  = {Journal of Voice},
  year     = {2013},
  volume   = {27},
  number   = {2},
  pages    = {258.e1 - 258.e13},
  issn     = {0892-1997},
  abstract = {Summary This study investigated changes perceived by singers and listeners after the singers had vocally warmed up. The study used a repeated measures within-subject design to assess changes in vibrato quality from pre (nonwarmed-up voice) to post (warmed-up voice) test. Intraclass correlation coefficients were calculated to assess singers' self-ratings pre- and posttest and intra- and interlistener rater reliability. Twelve classically trained female singers recorded and self-rated their performance of an eight bar solo before and after 25 minutes of vocal warm-up exercises. Six experienced listeners assessed the vocal samples for pre- to posttest differences in tone quality and for each singer's warm-up condition. Perceptual judgements were also compared with pre- to posttest changes in vibrato. All singers perceived significant changes in tone quality, psychophysiological factors, proprioceptive feedback and technical command. Significant pre- to posttest differences in tone quality and correct appraisal of the singer's warm-up condition from most of the listeners were only observed for singers who moderated extremely fast or extremely slow vibrato after warming up. The findings reveal the divide between listeners' and singers' perceptions of the warmed-up voice and highlight the importance of enhanced vibrato quality to listener perception of an improvement in vocal quality.},
  doi      = {https://doi.org/10.1016/j.jvoice.2012.12.001},
  file     = {:Singer and Listener Perception of Vocal Warm-Up.pdf:PDF},
  keywords = {Vocal warm-up, Perception, Tone quality, Vibrato},
  url      = {http://www.sciencedirect.com/science/article/pii/S0892199712002081},
}

@InProceedings{6637747,
  author    = {E. Molina and I. Barbancho and E. Gómez and A. M. Barbancho and L. J. Tardón},
  title     = {Fundamental frequency alignment vs. note-based melodic similarity for singing voice assessment},
  booktitle = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
  year      = {2013},
  pages     = {744-748},
  month     = {May},
  abstract  = {This paper presents a generic approach for automatic singing assessment for basic singing levels. The system provides the user with a set of intonation, rhythm and overall ratings obtained by measuring the similarity of the sung melody and a target performance. Two different similarity approaches are discussed: f0 curve alignment through Dynamic Time Warping (DTW), and singing transcription plus note-level similarity. From these two approaches, we extract different intonation and rhythm similarity measures which are combined through quadratic polynomial regression analysis in order to fit the judgement of 4 trained musicians on 27 performances. The results show that the proposed system is suitable for automatic singing voice rating and that DTW based measures are specially simple and effective for intonation and rhythm assessment.},
  doi       = {10.1109/ICASSP.2013.6637747},
  file      = {:6637747 - Fundamental frequency alignment vs. note-based melodic similarity for singing voice assessment.pdf:PDF},
  issn      = {1520-6149},
  keywords  = {acoustic signal processing;music;quadratic programming;regression analysis;speech synthesis;DTW based measure;automatic singing voice assessment;automatic singing voice rating;dynamic time warping;fundamental frequency alignment;intonation assessment;intonation similarity measure;note-based melodic similarity;quadratic polynomial regression analysis;rhythm assessment;rhythm similarity measure;similarity approach;singing transcription plus note level similarity measure;sung melody;target performance;Correlation;Frequency measurement;Measurement uncertainty;Polynomials;Reliability;Rhythm;Speech;automatic transcription;melodic similarity;score alignment;singing assessment;singing voice},
}

@InProceedings{7966037,
  author        = {J. Böhm and F. Eyben and M. Schmitt and H. Kosch and B. Schuller},
  title         = {Seeking the SuperStar: Automatic assessment of perceived singing quality},
  booktitle     = {2017 International Joint Conference on Neural Networks (IJCNN)},
  year          = {2017},
  pages         = {1560-1569},
  month         = {May},
  __markedentry = {[xiaoc:1]},
  abstract      = {The quality of the singing voice is an important aspect of subjective, aesthetic perception of music. In this contribution, we propose a method to automatically assess perceived singing quality. We classify monophonic vocal recordings without accompaniment into one of three classes of singing quality. Unprocessed private and non-commercial recordings from a social media website are utilised. In addition to the user ratings given on the website, we let both subjects with and without a musical background annotate the samples. Building on musicological foundations, we define and extract acoustic parameters describing the quality of the sound, musical expression and intonation of the singing. Besides features which are already established in the field of Music Information Retrieval, such as loudness and mel-frequency cepstral coefficients, we propose and employ new types of features which are specific to intonation. For automatic classification by supervised machine learning methods, models predicting the subjective ratings and the user ratings on the social media website are learnt. We perform an exhaustive evaluation of both different classifiers and combinations of features. We show that the performance of automatic classification is close to that of human evaluators. Utilising support vector machines, an accuracy of classification of 55.4 %, based on the subjective ratings, and of 84.7 %, based on the user ratings of the social media website, are achieved.},
  doi           = {10.1109/IJCNN.2017.7966037},
  file          = {:7966037 - Seeking the SuperStar_ Automatic assessment of perceived singing quality.pdf:PDF;:7966037 - Seeking the SuperStar_ Automatic assessment of perceived singing quality.pdf:PDF},
  keywords      = {acoustic signal processing;information retrieval;music;signal classification;acoustic parameter extraction;automatic assessment;automatic classification;monophonic vocal recordings;music information retrieval;perceived singing quality;singing voice;social media Website;supervised machine learning methods;support vector machines;Feature extraction;Frequency measurement;Rhythm;Social network services;Standards;Timbre},
}

@Article{JCAL:JCAL178,
  author    = {Hoppe, D. and Sadakata, M. and Desain, P.},
  title     = {Development of real-time visual feedback assistance in singing training: a review},
  journal   = {Journal of Computer Assisted Learning},
  year      = {2006},
  volume    = {22},
  number    = {4},
  pages     = {308--316},
  issn      = {1365-2729},
  abstract  = {Abstract
					 Four real-time visual feedback computer tools for singing lessons (Singad, Albert, Sing & See, and WinSINGAD), and the research carried out to evaluate the usefulness of these systems are reviewed in this article. We report on the development of user-functions and the usability of these computer-assisted learning tools. Both quantitative and qualitative studies confirm the efficiency of real-time visual feedback in improving singing abilities. Having addressed these findings, we suggest further quantitative investigations of (1) the detailed effect of visual feedback on performance accuracy and on the learning process, and (2) the interactions between improvement of musical performance and the type of visual feedback and the amount of information it presents, the skill level of the user and the teacher's role.},
  doi       = {10.1111/j.1365-2729.2006.00178.x},
  keywords  = {real-time visual feedback, review, singing education},
  publisher = {Blackwell Publishing Ltd},
  url       = {http://dx.doi.org/10.1111/j.1365-2729.2006.00178.x},
}

@InProceedings{7071545,
  author    = {A. Sasou},
  title     = {Singing voice recognition considering high-pitched and prolonged sounds},
  booktitle = {2006 14th European Signal Processing Conference},
  year      = {2006},
  pages     = {1-4},
  month     = {Sept},
  abstract  = {A conventional Large Vocabulary Continuous Speech Recognition (LVCSR) system has difficulty recognizing singing voices accurately because both the high-pitched and prolonged sounds of singing voices tend to degrade its recognition accuracy. We previously described an Auto-Regressive Hidden Markov Model (AR-HMM) and an accompanying parameter estimation method. We demonstrated that the AR-HMM accurately estimated the characteristics of both articulatory systems and excitation signals from high-pitched speech. In this paper, we describe an AR-HMM applied to feature extraction from singing voices and propose a prolonged-sound detection and elimination method.},
  issn      = {2219-5491},
  keywords  = {feature extraction;hidden Markov models;parameter estimation;speech recognition;AR-HMM;LVCSR system;articulatory system;autoregressive hidden Markov model;elimination method;excitation signal;feature extraction;large vocabulary continuous speech recognition system;parameter estimation method;prolonged-sound detection;singing voice recognition;Abstracts;Hidden Markov models;Mel frequency cepstral coefficient;Single photon emission computed tomography;Speech},
}

@InProceedings{7078515,
  author        = {O. Turk and L. M. Arslan},
  title         = {Software tools for speech therapy and voice quality monitoring},
  booktitle     = {2005 13th European Signal Processing Conference},
  year          = {2005},
  pages         = {1-4},
  month         = {Sept},
  __markedentry = {[xiaoc:1]},
  abstract      = {This study focuses on the development of software tools integrated with speech processing technology for speech pathologists and for patients with speech/language disorders and voice quality problems. An integrated interface called CATSEAR is under development for database collection, data analysis, therapy design, and patient monitoring. Automatic assessment techniques using pattern recognition algorithms enable the speech pathologist to employ objective criteria during speech therapy as well as guide the patients when the pathologist is not available. The performance of the patient can be monitored over time. CATSEAR enables sharing of databases among speech therapists with remote collaboration and pre-recorded analysis facilities. It can also be used for relatively mild disorders like mispronunciation and for singing voice training.},
  file          = {:07078515 - SOFTWARE TOOLS FOR SPEECH THERAPY AND VOICE QUALITY MONITORING.pdf:PDF},
  keywords      = {data analysis;handicapped aids;patient monitoring;patient treatment;software packages;speech processing;user interfaces;CATSEAR;automatic assessment techniques;data analysis;database collection;patient monitoring;pattern recognition algorithms;singing voice training;software tools;speech pathologists;speech processing technology;speech therapy;therapy design;voice quality monitoring;voice quality problems;Databases;Hidden Markov models;Medical treatment;Monitoring;Speech;Speech processing;Speech recognition},
}

@Conference{letowski1989sound,
  author    = {Letowski, Tomasz},
  title     = {Sound Quality Assessment: Concepts and Criteria},
  booktitle = {Audio Engineering Society Convention 87},
  year      = {1989},
  month     = {Oct},
  url       = {http://www.aes.org/e-lib/browse.cfm?elib=5869},
}

@Article{jekosch1999meaning,
  author    = {Jekosch, Ute},
  title     = {Meaning in the context of sound quality assessment},
  journal   = {Acta Acustica united with Acustica},
  year      = {1999},
  volume    = {85},
  number    = {5},
  pages     = {681--684},
  publisher = {S. Hirzel Verlag},
}

@InProceedings{6637745,
  author    = {C. Y. Sha and Y. H. Yang and Y. C. Lin and H. H. Chen},
  title     = {Singing voice timbre classification of Chinese popular music},
  booktitle = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
  year      = {2013},
  pages     = {734-738},
  month     = {May},
  abstract  = {Singing voice plays an important role in the listening experience of music. In this paper, we propose to classify popular music by the timbre quality of the singing voice. Specifically, we adopt six singing voice timbre classes as the taxonomy and build a new data set, KKTIC, that contains the expert annotations of 387 Chinese popular songs. To build an automatic classifier, we resort to signal processing and machine learning techniques and extract a number of singing voice-related features such as vibrato and harmonic-to-noise ratio. We also propose the use of vocal segment detection and singing voice separation as preprocessing steps. Our evaluation identifies the relevant acoustic features and validates the importance of these preprocessing steps. The accuracy in timbre classification reaches 79.84% in a five-fold stratified cross validation.},
  doi       = {10.1109/ICASSP.2013.6637745},
  file      = {:Singing voice timbre classification of Chinese popular music.pdf:PDF},
  issn      = {1520-6149},
  keywords  = {music;signal classification;speech synthesis;Chinese popular music;KKTIC;automatic classifier;five-fold stratified cross validation;harmonic-to-noise ratio;machine learning techniques;signal processing;singing voice separation;singing voice timbre classification;singing voice-related features;timbre quality;vibrato;vocal segment detection;Accuracy;Feature extraction;Instruments;Signal processing;Timbre;Singing voice timbre;music information retrieval;singing voice separation;vocal segment detection},
}

@InProceedings{6176860,
  author    = {M. V. Jha and P. Rao},
  title     = {Assessing vowel quality for singing evaluation},
  booktitle = {2012 National Conference on Communications (NCC)},
  year      = {2012},
  pages     = {1-5},
  month     = {Feb},
  abstract  = {The proper pronunciation of lyrics is an important component of vocal music. While automatic vowel classification has been widely studied for speech, a separate investigation of the methods is needed for singing due to the differences in acoustic properties between sung and spoken vowels. Acoustic features combining spectrum envelope and pitch are used with classifiers trained on sung vowels for classification of test vowels segmented from the audio of solo singing. Two different classifiers are tested, viz., Gaussian Mixture Models (GMM) and Linear Regression, and observed to perform well on both male and female sung vowels.},
  doi       = {10.1109/NCC.2012.6176860},
  file      = {:Assessing vowel quality for singing evaluation.pdf:PDF},
  keywords  = {Gaussian processes;audio signal processing;regression analysis;GMM;Gaussian mixture models;acoustic features;acoustic properties;automatic vowel classification;linear regression;lyric pronunciation;singing evaluation;solo singing audio;spectrum envelope;spoken vowels;sung vowels;test vowel segmentation;vocal music component;vowel quality;Databases;Linear regression;Mel frequency cepstral coefficient;Robustness;Support vector machine classification;Training;Vectors;GMM;Linear Regression;MFCC;Singing Voice;Vowel Classification;Vowel Quality},
}

@Report{asanuma2010measurement,
  author    = {Kazushi Asanuma, Satoshi Okazaki, Kazunori Itoh, Mizue Kayama, Masami Hashimoto, Makoto Otani},
  title     = {Measurement technique in local area of body surface for singing evaluation},
  year      = {2010},
  abstract  = {The quality of the singing can be confirmed from breathing and singing voice. The singing trainer observes breathing and singing voice, and trainer subjectively evaluates it. Breathing and singing voice are closely related to the movement of the body surface. Especially, the way to move mouth and mandible is important. This time, we measured the surface EMG of digastric muscle. Subject is eight men. As a result, we are able to confirm information on surface EMG for training of singing. Therefore, it becomes possible to evaluate singing voice objectively},
  boottitle = {IEICE Technical Report},
  number    = {211},
  pages     = {29--34},
  url       = {http://www.ieice.org/ken/paper/20100928P00t/eng/},
  volume    = {110},
}

@InProceedings{nakano2006automatic,
  author    = {Nakano, Tomoyasu and Goto, Masataka and Hiraga, Yuzuru},
  title     = {An automatic singing skill evaluation method for unknown melodies using pitch interval accuracy and vibrato features},
  booktitle = {Ninth International Conference on Spoken Language Processing},
  year      = {2006},
}

@InProceedings{6613986,
  author    = {T. Maka},
  title     = {Attributes of audio feature contours for automatic singing evaluation},
  booktitle = {2013 36th International Conference on Telecommunications and Signal Processing (TSP)},
  year      = {2013},
  pages     = {517-520},
  month     = {July},
  abstract  = {This paper concerns the automatic singing assessment by use of the audio feature contours. We show that statistical properties of feature trajectories can be exploited to create voice-training system to judge the quality of singing. We have analysed a large set of audio features, using three feature selection methods, in order to determine descriptors with high discrimination power. Obtained feature vectors are fed up into support vector machine (SVM) classifier with RBF kernel for classification of singing phrases. In the result, singing phrase is assigned to one of three quality categories. The proposed approach yields to comparable assessment accuracy with a group of voice experts, where taking specific properties of training phrases into account can improve the final accuracy.},
  doi       = {10.1109/TSP.2013.6613986},
  keywords  = {feature extraction;radial basis function networks;statistical analysis;support vector machines;RBF kernel;audio feature contours;automatic singing evaluation;statistical properties;support vector machine;voice experts;voice-training system;Accuracy;Databases;Frequency measurement;Kernel;Support vector machines;Trajectory;Vectors;audio classification;feature contours;feature selection;singing assessment;singing voice},
}

@Article{6084727,
  author   = {W. H. Tsai and H. C. Lee},
  title    = {Automatic Evaluation of Karaoke Singing Based on Pitch, Volume, and Rhythm Features},
  journal  = {IEEE Transactions on Audio, Speech, and Language Processing},
  year     = {2012},
  volume   = {20},
  number   = {4},
  pages    = {1233-1243},
  month    = {May},
  issn     = {1558-7916},
  abstract = {This study aims to develop an automatic singing evaluation system for Karaoke performances. Many Karaoke systems in the market today come with a scoring function. The addition of the feature enhances the entertainment appeal of the system due to the competitive nature of humans. The automatic Karaoke scoring mechanism to date, however, is still rudimentary, often giving inconsistent results with scoring by human raters. A cause of blunder arises from the fact that often only the singing volume is used as the evaluation criteria. To improve on the singing evaluation capabilities on Karaoke machines, this study exploits various acoustic features, including pitch, volume, and rhythm to assess a singing performance. We invited a number of singers having different levels of singing capabilities to record for Karaoke solo vocal samples. The performances were rated independently by four musicians, and then used in conjunction with additional Karaoke Video Compact Disk music for the training of our proposed system. Our experiment shows that the results of automatic singing evaluation are close to the human rating, where the Pearson product-moment correlation coefficient between them is 0.82.},
  doi      = {10.1109/TASL.2011.2174224},
  file     = {:6084727 - Automatic Evaluation of Karaoke Singing Based on Pitch, Volume, and Rhythm Features.pdf:PDF},
  keywords = {audio signal processing;music;Pearson product-moment correlation coefficient;automatic singing evaluation system;karaoke scoring mechanism;karaoke singing;karaoke video compact disk music;pitch features;rhythm features;volume features;Accuracy;Humans;Lead;Rhythm;Timbre;Accompaniment;Karaoke;singing evaluation;solo vocal},
}

@Article{tsai2015automatic,
  author  = {Tsai, Wei-Ho and Ma, Cin-Hao and Hsu, Yi-Po},
  title   = {Automatic Singing Performance Evaluation Using Accompanied Vocals as Reference Bases.},
  journal = {J. Inf. Sci. Eng.},
  year    = {2015},
  volume  = {31},
  number  = {3},
  pages   = {821--838},
}

@Article{takeuchi2009pitch,
  author  = {Takeuchi, Hideyo and Hoguro, Masahiro and Umezaki, Taizo},
  title   = {A Pitch Extraction Method with High Frequency Resolution for Singing Evaluation},
  journal = {IEEJ Transactions on Electronics, Information and Systems},
  year    = {2009},
  volume  = {129},
  pages   = {1889--1901},
}

@Article{takeuchi2010karaoke,
  author  = {Takeuchi, Hideyo and Hoguro, Masahiro and Umezaki, Taizo},
  title   = {A KARAOKE system singing Evaluation method that more closely matches human evaluation},
  journal = {IEEJ Transactions on Electronics, Information and Systems},
  year    = {2010},
  volume  = {130},
  pages   = {1042--1053},
}

@Article{SIUPSINSKIENE2011e177,
  author   = {Nora Siupsinskiene and Hugo Lycke},
  title    = {Effects of Vocal Training on Singing and Speaking Voice Characteristics in Vocally Healthy Adults and Children Based on Choral and Nonchoral Data},
  journal  = {Journal of Voice},
  year     = {2011},
  volume   = {25},
  number   = {4},
  pages    = {e177 - e189},
  issn     = {0892-1997},
  abstract = {Summary This prospective cross-sectional study examines the effects of voice training on vocal capabilities in vocally healthy age and gender differentiated groups measured by voice range profile (VRP) and speech range profile (SRP). Frequency and intensity measurements of the VRP and SRP using standard singing and speaking voice protocols were derived from 161 trained choir singers (21 males, 59 females, and 81 prepubescent children) and from 188 nonsingers (38 males, 89 females, and 61 children). When compared with nonsingers, both genders of trained adult and child singers exhibited increased mean pitch range, highest frequency, and VRP area in high frequencies (P<0.05). Female singers and child singers also showed significantly increased mean maximum voice intensity, intensity range, and total VRP area. The logistic regression analysis showed that VRP pitch range, highest frequency, maximum voice intensity, and maximum-minimum intensity range, and SRP slope of speaking curve were the key predictors of voice training. Age, gender, and voice training differentiated norms of VRP and SRP parameters are presented. Significant positive effect of voice training on vocal capabilities, mostly singing voice, was confirmed. The presented norms for trained singers, with key parameters differentiated by gender and age, are suggested for clinical practice of otolaryngologists and speech-language pathologists.},
  doi      = {https://doi.org/10.1016/j.jvoice.2010.03.010},
  keywords = {Quantitative voice assessment, Voice range profile, Speech range profile, Voice training},
  url      = {http://www.sciencedirect.com/science/article/pii/S0892199710000500},
}

@Article{guthmiller1986goals,
  author    = {Guthmiller, John Harold},
  title     = {The goals of vocalization: Developing healthy voices and the potential for expressive singing},
  journal   = {The Choral Journal},
  year      = {1986},
  volume    = {26},
  number    = {7},
  pages     = {13--15},
  publisher = {JSTOR},
}

@InProceedings{7952261,
  author    = {J. F. Gemmeke and D. P. W. Ellis and D. Freedman and A. Jansen and W. Lawrence and R. C. Moore and M. Plakal and M. Ritter},
  title     = {Audio Set: An ontology and human-labeled dataset for audio events},
  booktitle = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2017},
  pages     = {776-780},
  month     = {March},
  abstract  = {Audio event recognition, the human-like ability to identify and relate sounds from audio, is a nascent problem in machine perception. Comparable problems such as object detection in images have reaped enormous benefits from comprehensive datasets - principally ImageNet. This paper describes the creation of Audio Set, a large-scale dataset of manually-annotated audio events that endeavors to bridge the gap in data availability between image and audio research. Using a carefully structured hierarchical ontology of 632 audio classes guided by the literature and manual curation, we collect data from human labelers to probe the presence of specific audio classes in 10 second segments of YouTube videos. Segments are proposed for labeling using searches based on metadata, context (e.g., links), and content analysis. The result is a dataset of unprecedented breadth and size that will, we hope, substantially stimulate the development of high-performance audio event recognizers.},
  doi       = {10.1109/ICASSP.2017.7952261},
  file      = {:7952261 - Audio Set_ An ontology and human-labeled dataset for audio events.pdf:PDF},
  keywords  = {audio databases;audio signal processing;meta data;ontologies (artificial intelligence);video signal processing;Audio Set;YouTube videos;audio classes;audio event recognition;audio research;content analysis;context analysis;data availability;high-performance audio event recognizers;human-labeled dataset;image research;large-scale dataset;machine perception;manually-annotated audio events;metadata;structured hierarchical ontology;Birds;Labeling;Music;Ontologies;Taxonomy;Audio event detection;audio databases;data collection;sound ontology},
}

@InProceedings{7952248,
  author    = {D. Fourer and G. Peeters},
  title     = {Objective characterization of audio signal quality: Applications to music collection description},
  booktitle = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2017},
  pages     = {711-715},
  month     = {March},
  abstract  = {In this paper, we propose a set of audio features to describe the quality of an audio signal. Audio quality is here considered as being modified by the chain of processes/effects applied to the individual instrument tracks to obtain the final mix of a musical piece. Thus, the quality also depends on the mastering processes applied to the final mix or the signal degradation caused by MP3 compression. To evaluate our proposal, we created a large set of artificial mixes and also used real-world studio mixes. Using unsupervised and supervised classification methods, we show that our proposed audio features can detect the processing chain. Since this processing chain applied in professional studio has evolved over the years, we use our audio features to directly predict the decade during which a music track was recorded.},
  doi       = {10.1109/ICASSP.2017.7952248},
  file      = {:7952248 - Objective characterization of audio signal quality_ Applications to music collection description.pdf:PDF},
  keywords  = {audio signal processing;data compression;musical instruments;signal classification;MP3 compression;audio signal quality;music collection description;music track;musical piece;signal degradation;unsupervised-supervised classification methods;Digital audio players;Dynamic range;Feature extraction;Instruments;Multiple signal classification;Music;Support vector machines;audio quality;audio reverse-engineering;database indexing;music information retrieval;music remixing},
}

@InProceedings{7952585,
  author    = {K. Choi and G. Fazekas and M. Sandler and K. Cho},
  title     = {Convolutional recurrent neural networks for music classification},
  booktitle = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2017},
  pages     = {2392-2396},
  month     = {March},
  abstract  = {We introduce a convolutional recurrent neural network (CRNN) for music tagging. CRNNs take advantage of convolutional neural networks (CNNs) for local feature extraction and recurrent neural networks for temporal summarisation of the extracted features. We compare CRNN with three CNN structures that have been used for music tagging while controlling the number of parameters with respect to their performance and training time per sample. Overall, we found that CRNNs show a strong performance with respect to the number of parameter and training time, indicating the effectiveness of its hybrid structure in music feature extraction and feature summarisation.},
  doi       = {10.1109/ICASSP.2017.7952585},
  file      = {:7952585 - Convolutional recurrent neural networks for music classification.pdf:PDF},
  keywords  = {feature extraction;feedforward neural nets;music;recurrent neural nets;signal classification;CNN structures;CRNN;convolutional recurrent neural networks;feature summarisation;hybrid structure;local feature extraction;music classification;music feature extraction;music tagging;parameter time;recurrent neural networks;temporal summarisation;training time;Convolution;Feature extraction;Kernel;Recurrent neural networks;Tagging;Training;Two dimensional displays;convolutional neural networks;music classification;recurrent neural networks},
}

@Article{7993036,
  author   = {Y. Qian and N. Chen and H. Dinkel and Z. Wu},
  title    = {Deep Feature Engineering for Noise Robust Spoofing Detection},
  journal  = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year     = {2017},
  volume   = {25},
  number   = {10},
  pages    = {1942-1955},
  month    = {Oct},
  issn     = {2329-9290},
  abstract = {Spoofing detection for automatic speaker verification (ASV) aims to discriminate between genuine and spoofed speech. This topic has received increased attentions recently due to safety concerns with deploying an ASV system. While the performance of spoofing detection has improved significantly in clean condition in recent studies, the performance degrades dramatically in noisy conditions. To address this issue, in this paper, we propose to extract robust and discriminative deep features by using deep learning techniques for spoofing detection. In particular, we employ deep feedforward, recurrent, and convolutional neural networks to extract discriminative features. We also introduce multicondition training, noise-aware training, and annealed dropout training to make neural networks more robust against noise and to avoid overfitting to specific spoofing attacks and noise types. The proposed neural networks and training techniques are combined into a single framework for spoofing detection. Experimental evaluation is carried out on a noisy version of the standard ASVspoof 2015 corpus, including both additive noisy and reverberant scenarios. Experimental results confirm that the proposed system dramatically decreases averaged equal error rates from 19.1% and 22.6% to 3.2% and 5.1% for seen and unseen noisy conditions, respectively.},
  doi      = {10.1109/TASLP.2017.2732162},
  file     = {:7993036 - Deep Feature Engineering for Noise Robust Spoofing Detection.pdf:PDF},
  keywords = {feature extraction;feedforward neural nets;recurrent neural nets;speaker recognition;additive noisy;annealed dropout training;automatic speaker verification;convolutional neural networks;deep feature engineering;deep feedforward neural networks;deep learning techniques;discriminative deep feature extraction;multicondition training;noise robust spoofing detection;noise-aware training;recurrent neural networks;reverberant scenarios;robust deep feature extraction;standard ASV spoof 2015 corpus;Feature extraction;Neural networks;Noise measurement;Noise robustness;Robustness;Speech;Training;Deep learning;deep features;noise robust;speaker verification;spoofing detection},
}

@Article{8114356,
  author   = {M. Borsky and D. D. Mehta and J. H. Van Stan and J. Gudnason},
  title    = {Modal and Nonmodal Voice Quality Classification Using Acoustic and Electroglottographic Features},
  journal  = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year     = {2017},
  volume   = {25},
  number   = {12},
  pages    = {2281-2291},
  month    = {Dec},
  issn     = {2329-9290},
  abstract = {The goal of this study was to investigate the performance of different feature types for voice quality classification using multiple classifiers. The study compared the COVAREP feature set; which included glottal source features, frequency warped cepstrum, and harmonic model features; against the mel-frequency cepstral coefficients (MFCCs) computed from the acoustic voice signal, acoustic-based glottal inverse filtered (GIF) waveform, and electroglottographic (EGG) waveform. Our hypothesis was that MFCCs can capture the perceived voice quality from either of these three voice signals. Experiments were carried out on recordings from 28 participants with normal vocal status who were prompted to sustain vowels with modal and nonmodal voice qualities. Recordings were rated by an expert listener using the Consensus Auditory-Perceptual Evaluation of Voice (CAPE-V), and the ratings were transformed into a dichotomous label (presence or absence) for the prompted voice qualities of modal voice, breathiness, strain, and roughness. The classification was done using support vector machines, random forests, deep neural networks, and Gaussian mixture model classifiers, which were built as speaker independent using a leave-one-speaker-out strategy. The best classification accuracy of 79.97% was achieved for the full COVAREP set. The harmonic model features were the best performing subset, with 78.47% accuracy, and the static+dynamic MFCCs scored at 74.52%. A closer analysis showed that MFCC and dynamic MFCC features were able to classify modal, breathy, and strained voice quality dimensions from the acoustic and GIF waveforms. Reduced classification performance was exhibited by the EGG waveform.},
  doi      = {10.1109/TASLP.2017.2759002},
  file     = {:8114356 - Modal and Nonmodal Voice Quality Classification Using Acoustic and Electroglottographic Features.pdf:PDF},
  keywords = {Gaussian processes;acoustic signal processing;cepstral analysis;feature extraction;mixture models;neural nets;signal classification;speech processing;support vector machines;COVAREP feature set;COVAREP set;EGG waveform;GIF waveforms;Gaussian mixture model classifiers;acoustic features;acoustic voice signal;acoustic waveforms;acoustic-based glottal inverse;classification accuracy;consensus auditory-perceptual evaluation;dynamic MFCC features;electroglottographic features;electroglottographic waveform;glottal source features;harmonic model features;mel-frequency cepstral coefficients;modal voice qualities;modal voice quality classification;multiple classifiers;nonmodal voice qualities;normal vocal status;perceived voice quality;reduced classification performance;static+dynamic MFCC;voice quality classification;voice quality dimensions;voice signals;Acoustics;Auditory system;Biology;Classification;Electrocardiography;Harmonic analysis;Mel frequency cepstral coefficient;Signal processing;COVAREP;Voice quality assessment;acoustics;consensus auditory-perceptual evaluation of voice;electroglottograph;glottal glottal inverse filtering;mel-frequency cepstral coefficients;modal voice;non-modal voice},
}

@InProceedings{7952175,
  author    = {A. Nagathil and J. W. Schlattmann and K. Neumann and R. Martin},
  title     = {A feature-based linear regression model for predicting perceptual ratings of music by cochlear implant listeners},
  booktitle = {2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2017},
  pages     = {346-350},
  month     = {March},
  abstract  = {While speech quality and intelligibility prediction methods for normal-hearing and hearing-impaired listeners have found a lot of attention as a cost-saving complement to listening tests, analogous procedures for music signals are still rare. In this paper a method is proposed for predicting perceptual ratings of music as obtained by cochlear implant (CI) listeners. For this purpose a listening test with CI listeners was conducted, who were asked to provide their ratings for music excerpts on different scales. It is shown that principal component regression (PCR) is a suitable tool to model and accurately predict the median ratings of the CI listeners using timbre and pitch related signal features as predictor variables. These features describe signal characteristics such as high-frequency energy, spectral bandwidth and roughness. The proposed prediction model is a first step towards an instrumental evaluation procedure for music processing algorithms in hearing devices.},
  doi       = {10.1109/ICASSP.2017.7952175},
  file      = {:7952175 - A feature-based linear regression model for predicting perceptual ratings of music by cochlear implant listeners.pdf:PDF},
  keywords  = {cochlear implants;music;principal component analysis;regression analysis;speech intelligibility;speech processing;CI listeners;PCR;cochlear implant listeners;cost-saving complement;feature-based linear regression model;hearing devices;hearing-impaired listeners;instrumental evaluation procedure;listening tests;music excerpts;music processing algorithms;music signals;normal-hearing listeners;perceptual ratings;pitch related signal features;predictor variables;principal component regression;signal characteristics;speech intelligibility prediction;speech quality prediction;timbre related signal features;Auditory system;Indexes;Multiple signal classification;Predictive models;Speech;Timbre;Music;cochlear implants;predictive models;principal component analysis;regression},
}

@InProceedings{Li:2017:RRC:3057039.3057049,
  author    = {Li, Juan and Wang, Yinrui and Yang, Xinyu},
  title     = {Regional Recognition of Chinese Folk Songs Based on LSD Audio Segmentation Algorithm},
  booktitle = {Proceedings of the 9th International Conference on Computer and Automation Engineering},
  year      = {2017},
  series    = {ICCAE '17},
  pages     = {60--65},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3057049},
  doi       = {10.1145/3057039.3057049},
  file      = {:Li_2017_RRC_3057039.3057049 - Regional Recognition of Chinese Folk Songs Based on LSD Audio Segmentation Algorithm.pdf:PDF},
  isbn      = {978-1-4503-4809-6},
  keywords  = {HMM-GMM, Regional recognition, audio segmentation, stabilities},
  location  = {Sydney, Australia},
  numpages  = {6},
  url       = {http://doi.acm.org/10.1145/3057039.3057049},
}

@Article{han2016sparse,
  author    = {Han, Yoonchang and Lee, Subin and Nam, Juhan and Lee, Kyogu},
  title     = {Sparse feature learning for instrument identification: Effects of sampling and pooling methods},
  journal   = {The Journal of the Acoustical Society of America},
  year      = {2016},
  volume    = {139},
  number    = {5},
  pages     = {2290--2298},
  file      = {:Sparse feature learning for instrument identification Effects of sampling and pooling methods.pdf:PDF},
  publisher = {ASA},
}

@Article{Santacruz2017,
  author   = {Santacruz, Jos{\'e} L. and Tard{\'o}n, Lorenzo J. and Barbancho, Isabel and Barbancho, Ana M. and Molina, Emilio},
  title    = {VOICE2TUBA: transforming singing voice into a musical instrument},
  journal  = {Multimedia Tools and Applications},
  year     = {2017},
  volume   = {76},
  number   = {7},
  pages    = {9855--9875},
  month    = {Apr},
  issn     = {1573-7721},
  abstract = {In this paper, a scheme to synthesize and convert singing voice into tuba sound is presented. First, our method estimates the fundamental frequency (F                        0) and the aperiodicity of a monophonic audio signal, in order to obtain the pitch and volume variations of human voice. Then, the parameters extracted are used to generate a musical excerpt emulating a certain musical instrument (tuba) in such a way that the melody resembles the original sung song. To this end, two different generation approaches are devised. One of them is based on additive signal synthesis from harmonic amplitudes. The other one converts the F                        0 curve into a MIDI stream, in order to allow the play back with a virtual tuba.},
  day      = {01},
  doi      = {10.1007/s11042-016-3582-0},
  file     = {:VOICE2TUBA transforming singing voice into.pdf:PDF},
  url      = {https://doi.org/10.1007/s11042-016-3582-0},
}

@Article{doi:10.1080/09298215.2015.1132737,
  author    = {Kai Siedenburg and Ichiro Fujinaga and Stephen McAdams},
  title     = {A Comparison of Approaches to Timbre Descriptors in Music Information Retrieval and Music Psychology},
  journal   = {Journal of New Music Research},
  year      = {2016},
  volume    = {45},
  number    = {1},
  pages     = {27-41},
  abstract  = { A curious divide characterizes the usage of audio descriptors for timbre research in music information research (MIR) and music psychology. While MIR uses a multitude of audio descriptors for tasks such as automatic instrument classification, only a highly constrained set is used to describe the physical correlates of timbre perception in parts of music psychology. We argue that this gap is not coincidental and results from the differences in the two fields’ methodologies, their epistemic groundwork, and research goals. This paper lays out perspectives on the emergence of the divide and reviews studies in both fields with regards to divergences in research methods and goals. We discuss new representations for spectro-temporal modulations in MIR and psychology, and compare approaches to spectral envelope description in depth. Finally, we will propose that the interdisciplinary discourse on the computational modelling of music requires negotiations about the roles of scientific evaluation criteria. },
  doi       = {10.1080/09298215.2015.1132737},
  eprint    = {https://doi.org/10.1080/09298215.2015.1132737},
  file      = {:A Comparison of Approaches to Timbre Descriptors in Music Information Retrieval and Music Psychology.pdf:PDF},
  publisher = {Routledge},
  url       = { 
        https://doi.org/10.1080/09298215.2015.1132737
    
},
}

@Article{thoret2016perceptually,
  author    = {Thoret, Etienne and Depalle, Philippe and McAdams, Stephen},
  title     = {Perceptually salient spectrotemporal modulations for recognition of sustained musical instruments},
  journal   = {The Journal of the Acoustical Society of America},
  year      = {2016},
  volume    = {140},
  number    = {6},
  pages     = {EL478--EL483},
  file      = {:Perceptually salient spectrotemporal modulations for recognition of sustained musical instruments.pdf:PDF},
  publisher = {ASA},
}

@Article{shabtai2017generation,
  author    = {Shabtai, Noam R and Behler, Gottfried and Vorl{\"a}nder, Michael and Weinzierl, Stefan},
  title     = {Generation and analysis of an acoustic radiation pattern database for forty-one musical instruments},
  journal   = {The Journal of the Acoustical Society of America},
  year      = {2017},
  volume    = {141},
  number    = {2},
  pages     = {1246--1256},
  file      = {:Generation and analysis of an acoustic radiation pattern database for forty-one musical instruments.pdf:PDF},
  publisher = {ASA},
}

@InProceedings{McFee:2012:MSD:2187980.2188222,
  author    = {McFee, Brian and Bertin-Mahieux, Thierry and Ellis, Daniel P.W. and Lanckriet, Gert R.G.},
  title     = {The Million Song Dataset Challenge},
  booktitle = {Proceedings of the 21st International Conference on World Wide Web},
  year      = {2012},
  series    = {WWW '12 Companion},
  pages     = {909--916},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {2188222},
  doi       = {10.1145/2187980.2188222},
  file      = {:McFee_2012_MSD_2187980.2188222 - The Million Song Dataset Challenge.pdf:PDF},
  isbn      = {978-1-4503-1230-1},
  keywords  = {music information retrieval, recommender systems},
  location  = {Lyon, France},
  numpages  = {8},
  url       = {http://doi.acm.org/10.1145/2187980.2188222},
}

@Article{doi:10.1121/1.426435,
  author  = {Bozena Kostek and Piotr Suchomski},
  title   = {Multimedia database of musical instrument sounds},
  journal = {The Journal of the Acoustical Society of America},
  year    = {1999},
  volume  = {105},
  number  = {2},
  pages   = {1358-1358},
  doi     = {10.1121/1.426435},
  eprint  = {https://doi.org/10.1121/1.426435},
  url     = { 
        https://doi.org/10.1121/1.426435
    
},
}

@Article{doi:10.1121/1.4785094,
  author  = {K. Anthony Hoover and Andrew Carballeira and Pam Harght and Sam Ortallono},
  title   = {Musical instrument sound levels},
  journal = {The Journal of the Acoustical Society of America},
  year    = {2004},
  volume  = {116},
  number  = {4},
  pages   = {2529-2529},
  doi     = {10.1121/1.4785094},
  eprint  = {https://doi.org/10.1121/1.4785094},
  url     = { 
        https://doi.org/10.1121/1.4785094
    
},
}

@Article{doi:10.1121/1.5014036,
  author  = {A. Almeida and W. Li and J. R. Smith and J. Wolfe},
  title   = {The mechanism producing initial transients on the clarinet},
  journal = {The Journal of the Acoustical Society of America},
  year    = {2017},
  volume  = {142},
  number  = {6},
  pages   = {3376-3386},
  doi     = {10.1121/1.5014036},
  eprint  = {https://doi.org/10.1121/1.5014036},
  url     = { 
        https://doi.org/10.1121/1.5014036
    
},
}

@Article{produce2013acoustic,
  author = {Produce, Why Do Different Sound Sources},
  title  = {Acoustic Timbre Recognition},
  year   = {2013},
  file   = {:Acoustic Timbre Recognition.pdf:PDF},
}

@Article{article,
  author = {Shi, Yuan-Yuan and Wen, Xue and She, Bin},
  title  = {ESRD03 Database and the Labeling for Environmental Sound Recognition},
  year   = {2018},
  month  = {02},
  file   = {:ESRD03_Database_and_the_Labeling_for_Environmental.pdf:PDF},
}

@InProceedings{goto2002rwc,
  author    = {Goto, Masataka and Hashiguchi, Hiroki and Nishimura, Takuichi and Oka, Ryuichi},
  title     = {RWC Music Database: Popular, Classical and Jazz Music Databases.},
  booktitle = {ISMIR},
  year      = {2002},
  volume    = {2},
  pages     = {287--288},
  file      = {:RWC Music Database Popular, Classical and Jazz Music Databases.pdf:PDF},
}

@InProceedings{Kosugi:2000:PQS:354384.354520,
  author    = {Kosugi, Naoko and Nishihara, Yuichi and Sakata, Tetsuo and Yamamuro, Masashi and Kushima, Kazuhiko},
  title     = {A Practical Query-by-humming System for a Large Music Database},
  booktitle = {Proceedings of the Eighth ACM International Conference on Multimedia},
  year      = {2000},
  series    = {MULTIMEDIA '00},
  pages     = {333--342},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {354520},
  doi       = {10.1145/354384.354520},
  file      = {:Kosugi_2000_PQS_354384.354520 - A Practical Query-by-humming System for a Large Music Database.pdf:PDF},
  isbn      = {1-58113-198-4},
  location  = {Marina del Rey, California, USA},
  numpages  = {10},
  url       = {http://doi.acm.org/10.1145/354384.354520},
}

@InProceedings{Uitdenbogerd:1999:MMT:319463.319470,
  author    = {Uitdenbogerd, Alexandra and Zobel, Justin},
  title     = {Melodic Matching Techniques for Large Music Databases},
  booktitle = {Proceedings of the Seventh ACM International Conference on Multimedia (Part 1)},
  year      = {1999},
  series    = {MULTIMEDIA '99},
  pages     = {57--66},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {319470},
  doi       = {10.1145/319463.319470},
  file      = {:Uitdenbogerd_1999_MMT_319463.319470 - Melodic Matching Techniques for Large Music Databases.pdf:PDF},
  isbn      = {1-58113-151-8},
  location  = {Orlando, Florida, USA},
  numpages  = {10},
  url       = {http://doi.acm.org/10.1145/319463.319470},
}

@InProceedings{typke2005survey,
  author       = {Typke, Rainer and Wiering, Frans and Veltkamp, Remco C},
  title        = {A survey of music information retrieval systems},
  booktitle    = {Proc. 6th International Conference on Music Information Retrieval},
  year         = {2005},
  pages        = {153--160},
  organization = {Queen Mary, University of London},
  file         = {:Typke_05_Survey_of_Music_Information_Retrieval_Systems.pdf:PDF},
}

@InProceedings{bittner2014medleydb,
  author    = {Bittner, Rachel M and Salamon, Justin and Tierney, Mike and Mauch, Matthias and Cannam, Chris and Bello, Juan Pablo},
  title     = {MedleyDB: A Multitrack Dataset for Annotation-Intensive MIR Research.},
  booktitle = {ISMIR},
  year      = {2014},
  volume    = {14},
  pages     = {155--160},
  file      = {:bittner_medleydb_ismir2014.pdf:PDF},
}

@Article{Salamon2013,
  author   = {Salamon, Justin and Serr{\`a}, Joan and G{\'o}mez, Emilia},
  title    = {Tonal representations for music retrieval: from version identification to query-by-humming},
  journal  = {International Journal of Multimedia Information Retrieval},
  year     = {2013},
  volume   = {2},
  number   = {1},
  pages    = {45--58},
  month    = {Mar},
  issn     = {2192-662X},
  abstract = {In this study we compare the use of different music representations for retrieving alternative performances of the same musical piece, a task commonly referred to as version identification. Given the audio signal of a song, we compute descriptors representing its melody, bass line and harmonic progression using state-of-the-art algorithms. These descriptors are then employed to retrieve different versions of the same musical piece using a dynamic programming algorithm based on nonlinear time series analysis. First, we evaluate the accuracy obtained using individual descriptors, and then we examine whether performance can be improved by combining these music representations (i.e. descriptor fusion). Our results show that whilst harmony is the most reliable music representation for version identification, the melody and bass line representations also carry useful information for this task. Furthermore, we show that by combining these tonal representations we can increase version detection accuracy. Finally, we demonstrate how the proposed version identification method can be adapted for the task of query-by-humming. We propose a melody-based retrieval approach, and demonstrate how melody representations extracted from recordings of a cappella singing can be successfully used to retrieve the original song from a collection of polyphonic audio. The current limitations of the proposed approach are discussed in the context of version identification and query-by-humming, and possible solutions and future research directions are proposed.},
  day      = {01},
  doi      = {10.1007/s13735-012-0026-0},
  file     = {:salamon-mmir.pdf:PDF},
  url      = {https://doi.org/10.1007/s13735-012-0026-0},
}

@InProceedings{goto2006aist,
  author    = {Goto, Masataka},
  title     = {AIST Annotation for the RWC Music Database.},
  booktitle = {ISMIR},
  year      = {2006},
  pages     = {359--360},
  file      = {:ISMIR2006goto.pdf:PDF},
}

@InProceedings{PikrakisFlamencoISMIR12,
  author    = {Pikrakis, A. and G{\'o}mez, F. and Oramas, S. and B{\'a}{\~n}ez, J. M. D. and Mora, J. and Escobar, F. and G{\'o}mez, E. and Salamon, J.},
  title     = {Tracking Melodic Patterns in Flamenco Singing by Analyzing Polyphonic Music Recordings},
  booktitle = {13th Int.~Soc.~for Music Info.~Retrieval Conf.},
  year      = {2012},
  address   = {Porto, Portugal},
  month     = {Oct.},
}

@Article{5153305,
  author   = {C. L. Hsu and J. S. R. Jang},
  title    = {On the Improvement of Singing Voice Separation for Monaural Recordings Using the MIR-1K Dataset},
  journal  = {IEEE Transactions on Audio, Speech, and Language Processing},
  year     = {2010},
  volume   = {18},
  number   = {2},
  pages    = {310-319},
  month    = {Feb},
  issn     = {1558-7916},
  abstract = {Monaural singing voice separation is an extremely challenging problem. While efforts in pitch-based inference methods have led to considerable progress in voiced singing voice separation, little attention has been paid to the incapability of such methods to separate unvoiced singing voice due to its in harmonic structure and weaker energy. In this paper, we proposed a systematic approach to identify and separate the unvoiced singing voice from the music accompaniment. We have also enhanced the performance of separating voiced singing via a spectral subtraction method. The proposed system follows the framework of computational auditory scene analysis (CASA) which consists of the segmentation stage and the grouping stage. In the segmentation stage, the input song signals are decomposed into small sensory elements in different time-frequency resolutions. The unvoiced sensory elements are then identified by Gaussian mixture models. The experimental results demonstrated that the quality of the separated singing voice is improved for both the unvoiced and voiced parts. Moreover, to deal with the problem of lack of a publicly available dataset for singing voice separation, we have constructed a corpus called MIR-1K (multimedia information retrieval lab, 1000 song clips) where all singing voices and music accompaniments were recorded separately. Each song clip comes with human-labeled pitch values, unvoiced sounds and vocal/non-vocal segments, and lyrics, as well as the speech recording of the lyrics.},
  doi      = {10.1109/TASL.2009.2026503},
  file     = {:5153305 - On the Improvement of Singing Voice Separation for Monaural Recordings Using the MIR-1K Dataset.pdf:PDF},
  keywords = {Gaussian processes;acoustic signal processing;music;recording;source separation;Gaussian mixture models;MIR-1K dataset;computational auditory scene analysis;grouping stage;harmonic structure;human-labeled pitch values;input song signal decomposition;monaural recordings;monaural singing voice separation;multimedia information retrieval lab;music accompaniment;segmentation stage;spectral subtraction method;speech recording;systematic approach;time-frequency resolutions;unvoiced sensory elements;Computational auditory scene analysis (CASA);singing voice separation;unvoiced sound separation},
}

@Article{4432652,
  author   = {D. Turnbull and L. Barrington and D. Torres and G. Lanckriet},
  title    = {Semantic Annotation and Retrieval of Music and Sound Effects},
  journal  = {IEEE Transactions on Audio, Speech, and Language Processing},
  year     = {2008},
  volume   = {16},
  number   = {2},
  pages    = {467-476},
  month    = {Feb},
  issn     = {1558-7916},
  abstract = {We present a computer audition system that can both annotate novel audio tracks with semantically meaningful words and retrieve relevant tracks from a database of unlabeled audio content given a text-based query. We consider the related tasks of content-based audio annotation and retrieval as one supervised multiclass, multilabel problem in which we model the joint probability of acoustic features and words. We collect a data set of 1700 human-generated annotations that describe 500 Western popular music tracks. For each word in a vocabulary, we use this data to train a Gaussian mixture model (GMM) over an audio feature space. We estimate the parameters of the model using the weighted mixture hierarchies expectation maximization algorithm. This algorithm is more scalable to large data sets and produces better density estimates than standard parameter estimation techniques. The quality of the music annotations produced by our system is comparable with the performance of humans on the same task. Our ldquoquery-by-textrdquo system can retrieve appropriate songs for a large number of musically relevant words. We also show that our audition system is general by learning a model that can annotate and retrieve sound effects.},
  doi      = {10.1109/TASL.2007.913750},
  keywords = {Gaussian processes;audio databases;audio signal processing;content-based retrieval;music;Gaussian mixture model;audio annotation;audio feature space;computer audition system;content-based retrieval;music retrieval;parameter estimation;query-by-text system;semantic annotation;sound effects;text-based query;Audio databases;Content based retrieval;Humans;Information analysis;Instruments;Music information retrieval;Natural languages;Parameter estimation;Spatial databases;Vocabulary;Audio annotation and retrieval;music information retrieval;semantic music analysis},
}

@Article{6155601,
  author   = {J. Salamon and E. Gomez},
  title    = {Melody Extraction From Polyphonic Music Signals Using Pitch Contour Characteristics},
  journal  = {IEEE Transactions on Audio, Speech, and Language Processing},
  year     = {2012},
  volume   = {20},
  number   = {6},
  pages    = {1759-1770},
  month    = {Aug},
  issn     = {1558-7916},
  abstract = {We present a novel system for the automatic extraction of the main melody from polyphonic music recordings. Our approach is based on the creation and characterization of pitch contours, time continuous sequences of pitch candidates grouped using auditory streaming cues. We define a set of contour characteristics and show that by studying their distributions we can devise rules to distinguish between melodic and non-melodic contours. This leads to the development of new voicing detection, octave error minimization and melody selection techniques. A comparative evaluation of the proposed approach shows that it outperforms current state-of-the-art melody extraction systems in terms of overall accuracy. Further evaluation of the algorithm is provided in the form of a qualitative error analysis and the study of the effect of key parameters and algorithmic components on system performance. Finally, we conduct a glass ceiling analysis to study the current limitations of the method, and possible directions for future work are proposed.},
  doi      = {10.1109/TASL.2012.2188515},
  file     = {:6155601 - Melody Extraction From Polyphonic Music Signals Using Pitch Contour Characteristics.pdf:PDF},
  keywords = {acoustic signal detection;audio recording;ceilings;error analysis;error statistics;feature extraction;music;sequences;auditory streaming cues;automatic melody extraction;glass ceiling analysis;key parameters components;melodic contours;melody selection techniques;nonmelodic contours;octave error minimization;pitch contour characteristics;polyphonic music recording;polyphonic music signals;qualitative error analysis;state-of-the-art melody extraction systems;time continuous pitch sequences;voice detection;Algorithm design and analysis;Humans;Instruments;Materials;Multiple signal classification;Speech;Speech processing;Audio content description;multi-pitch estimation;music information retrieval;pitch contour;predominant melody estimation},
}

@Article{7100934,
  author   = {D. Stowell and D. Giannoulis and E. Benetos and M. Lagrange and M. D. Plumbley},
  title    = {Detection and Classification of Acoustic Scenes and Events},
  journal  = {IEEE Transactions on Multimedia},
  year     = {2015},
  volume   = {17},
  number   = {10},
  pages    = {1733-1746},
  month    = {Oct},
  issn     = {1520-9210},
  abstract = {For intelligent systems to make best use of the audio modality, it is important that they can recognize not just speech and music, which have been researched as specific tasks, but also general sounds in everyday environments. To stimulate research in this field we conducted a public research challenge: the IEEE Audio and Acoustic Signal Processing Technical Committee challenge on Detection and Classification of Acoustic Scenes and Events (DCASE). In this paper, we report on the state of the art in automatically classifying audio scenes, and automatically detecting and classifying audio events. We survey prior work as well as the state of the art represented by the submissions to the challenge from various research groups. We also provide detail on the organization of the challenge, so that our experience as challenge hosts may be useful to those organizing challenges in similar domains. We created new audio datasets and baseline systems for the challenge; these, as well as some submitted systems, are publicly available under open licenses, to serve as benchmarks for further research in general-purpose machine listening.},
  doi      = {10.1109/TMM.2015.2428998},
  file     = {:07100934.pdf:PDF},
  keywords = {acoustic signal processing;knowledge based systems;speech recognition;DCASE;IEEE Audio and Acoustic Signal Processing Technical Committee;acoustic scenes classification;acoustic scenes detection;audio modality;intelligent systems;music;speech recognition;Event detection;Licenses;Microphones;Music;Speech;Speech recognition;Audio databases;event detection;machine intelligence;pattern recognition},
}

@Article{8123864,
  author   = {A. Mesaros and T. Heittola and E. Benetos and P. Foster and M. Lagrange and T. Virtanen and M. D. Plumbley},
  title    = {Detection and Classification of Acoustic Scenes and Events: Outcome of the DCASE 2016 Challenge},
  journal  = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year     = {2018},
  volume   = {26},
  number   = {2},
  pages    = {379-393},
  month    = {Feb},
  issn     = {2329-9290},
  abstract = {Public evaluation campaigns and datasets promote active development in target research areas, allowing direct comparison of algorithms. The second edition of the challenge on detection and classification of acoustic scenes and events (DCASE 2016) has offered such an opportunity for development of the state-of-the-art methods, and succeeded in drawing together a large number of participants from academic and industrial backgrounds. In this paper, we report on the tasks and outcomes of the DCASE 2016 challenge. The challenge comprised four tasks: acoustic scene classification, sound event detection in synthetic audio, sound event detection in real-life audio, and domestic audio tagging. We present each task in detail and analyze the submitted systems in terms of design and performance. We observe the emergence of deep learning as the most popular classification method, replacing the traditional approaches based on Gaussian mixture models and support vector machines. By contrast, feature representations have not changed substantially throughout the years, as mel frequency-based representations predominate in all tasks. The datasets created for and used in DCASE 2016 are publicly available and are a valuable resource for further research.},
  doi      = {10.1109/TASLP.2017.2778423},
  file     = {:08123864.pdf:PDF},
  keywords = {acoustic signal processing;audio signal processing;feature extraction;learning (artificial intelligence);signal classification;DCASE 2016 challenge;Gaussian mixture models;acoustic scene classification;acoustic scenes;classification method;deep learning;domestic audio tagging;public evaluation campaigns;sound event detection;support vector machines;synthetic audio;Acoustics;Event detection;Hidden Markov models;Speech;Speech processing;Tagging;Acoustic scene classification;audio datasets;pattern recognition;sound event detection},
}

@Article{7933045,
  author   = {V. Bisot and R. Serizel and S. Essid and G. Richard},
  title    = {Feature Learning With Matrix Factorization Applied to Acoustic Scene Classification},
  journal  = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year     = {2017},
  volume   = {25},
  number   = {6},
  pages    = {1216-1229},
  month    = {June},
  issn     = {2329-9290},
  abstract = {In this paper, we study the usefulness of various matrix factorization methods for learning features to be used for the specific acoustic scene classification (ASC) problem. A common way of addressing ASC has been to engineer features capable of capturing the specificities of acoustic environments. Instead, we show that better representations of the scenes can be automatically learned from time-frequency representations using matrix factorization techniques. We mainly focus on extensions including sparse, kernel-based, convolutive and a novel supervised dictionary learning variant of principal component analysis and nonnegative matrix factorization. An experimental evaluation is performed on two of the largest ASC datasets available in order to compare and discuss the usefulness of these methods for the task. We show that the unsupervised learning methods provide better representations of acoustic scenes than the best conventional hand-crafted features on both datasets. Furthermore, the introduction of a novel nonnegative supervised matrix factorization model and deep neural networks trained on spectrograms, allow us to reach further improvements.},
  doi      = {10.1109/TASLP.2017.2690570},
  file     = {:07933045.pdf:PDF},
  keywords = {acoustic signal processing;matrix decomposition;neural nets;principal component analysis;time-frequency analysis;unsupervised learning;ASC datasets;PCA;deep neural networks;feature learning;hand-crafted features;kernel-based variant;nonnegative supervised matrix factorization model;principal component analysis;specific acoustic scene classification problem;spectrograms;supervised dictionary learning variant;time-frequency representations;unsupervised learning methods;Dictionaries;Feature extraction;Hidden Markov models;Mel frequency cepstral coefficient;Sparse matrices;Time-frequency analysis;Acoustic scene classification;feature learning;matrix factorization},
}

@Article{7078982,
  author   = {D. Barchiesi and D. Giannoulis and D. Stowell and M. D. Plumbley},
  title    = {Acoustic Scene Classification: Classifying environments from the sounds they produce},
  journal  = {IEEE Signal Processing Magazine},
  year     = {2015},
  volume   = {32},
  number   = {3},
  pages    = {16-34},
  month    = {May},
  issn     = {1053-5888},
  abstract = {In this article, we present an account of the state of the art in acoustic scene classification (ASC), the task of classifying environments from the sounds they produce. Starting from a historical review of previous research in this area, we define a general framework for ASC and present different implementations of its components. We then describe a range of different algorithms submitted for a data challenge that was held to provide a general and fair benchmark for ASC techniques. The data set recorded for this purpose is presented along with the performance metrics that are used to evaluate the algorithms and statistical significance tests to compare the submitted methods.},
  doi      = {10.1109/MSP.2014.2326181},
  file     = {:07078982.pdf:PDF},
  keywords = {Acoustics;Classification algorithms;Feature extraction;Frequency measurement;Hidden Markov models;Image analysis;Signal processing algorithms},
}

@Article{6971128,
  author   = {A. Rakotomamonjy and G. Gasso},
  title    = {Histogram of Gradients of Time-Frequency Representations for Audio Scene Classification},
  journal  = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year     = {2015},
  volume   = {23},
  number   = {1},
  pages    = {142-153},
  month    = {Jan},
  issn     = {2329-9290},
  abstract = {Presents our entry to the Detection and Classification of Acoustic Scenes challenge. The approach we propose for classifying acoustic scenes is based on transforming the audio signal into a time-frequency representation and then in extracting relevant features about shapes and evolutions of time-frequency structures. These features are based on histogram of gradients that are subsequently fed to a multi-class linear support vector machines.},
  doi      = {10.1109/TASLP.2014.2375575},
  file     = {:06971128.pdf:PDF},
  keywords = {audio signal processing;gradient methods;support vector machines;time-frequency analysis;SVM;acoustic scenes;audio scene classification;audio signal;histogram of gradients;multiclass linear support vector machines;time-frequency representations;Feature extraction;Histograms;IEEE transactions;Mel frequency cepstral coefficient;Speech;Speech processing;Time-frequency analysis;Constant Q transform;histogram of gradient;support vector machines},
}

@InBook{Benetos2018bookapproach,
  chapter   = {Approaches to complex sound scene analysis},
  pages     = {215-242},
  title     = {Computational Analysis of Sound Scenes and Events},
  publisher = {Springer},
  year      = {2018},
  author    = {E. Benetos and D. Stowell and M. D. Plumbley},
  isbn      = {978-3-319-63450-0},
  file      = {:Benetos2018_Chapter_ApproachesToComplexSoundSceneA.pdf:PDF},
}

@Article{5975166,
  author   = {J. Mairal and F. Bach and J. Ponce},
  title    = {Task-Driven Dictionary Learning},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year     = {2012},
  volume   = {34},
  number   = {4},
  pages    = {791-804},
  month    = {April},
  issn     = {0162-8828},
  abstract = {Modeling data with linear combinations of a few elements from a learned dictionary has been the focus of much recent research in machine learning, neuroscience, and signal processing. For signals such as natural images that admit such sparse representations, it is now well established that these models are well suited to restoration tasks. In this context, learning the dictionary amounts to solving a large-scale matrix factorization problem, which can be done efficiently with classical optimization tools. The same approach has also been used for learning features from data for other purposes, e.g., image classification, but tuning the dictionary in a supervised way for these tasks has proven to be more difficult. In this paper, we present a general formulation for supervised dictionary learning adapted to a wide variety of tasks, and present an efficient algorithm for solving the corresponding optimization problem. Experiments on handwritten digit classification, digital art identification, nonlinear inverse image problems, and compressed sensing demonstrate that our approach is effective in large-scale settings, and is well suited to supervised and semi-supervised classification, as well as regression tasks for data that admit sparse representations.},
  doi      = {10.1109/TPAMI.2011.156},
  file     = {:05975166.pdf:PDF},
  keywords = {compressed sensing;data models;handwritten character recognition;image classification;image representation;image restoration;learning (artificial intelligence);matrix decomposition;regression analysis;classical optimization tools;compressed sensing;data modeling;digital art identification;handwritten digit classification;image classification;large-scale matrix factorization problem;learned dictionary;linear combinations;machine learning;natural images;neuroscience;nonlinear inverse image problems;regression tasks;restoration tasks;semisupervised classification;signal processing;sparse representations;supervised dictionary learning;task-driven dictionary learning;Cost function;Dictionaries;Machine learning;Sensors;Sparse matrices;Vectors;Basis pursuit;Lasso;compressed sensing.;dictionary learning;matrix factorization;semi-supervised learning;Algorithms;Databases, Factual;Humans;Pattern Recognition, Automated},
}

@InProceedings{1467360,
  author    = {N. Dalal and B. Triggs},
  title     = {Histograms of oriented gradients for human detection},
  booktitle = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
  year      = {2005},
  volume    = {1},
  pages     = {886-893 vol. 1},
  month     = {June},
  abstract  = {We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
  doi       = {10.1109/CVPR.2005.177},
  file      = {:8588935 - Histograms of Oriented Gradients for Human Detection.pdf:PDF},
  issn      = {1063-6919},
  keywords  = {feature extraction;gradient methods;object detection;object recognition;support vector machines;coarse spatial binning;contrast normalization;edge based descriptors;fine orientation binning;fine-scale gradients;gradient based descriptors;histograms of oriented gradients;human detection;linear SVM;overlapping descriptor;pedestrian database;robust visual object recognition;High performance computing;Histograms;Humans;Image databases;Image edge detection;Object detection;Object recognition;Robustness;Support vector machines;Testing},
}

@Article{5109766,
  author   = {S. Chu and S. Narayanan and C. C. J. Kuo},
  title    = {Environmental Sound Recognition With Time-frequency Audio Features},
  journal  = {IEEE Transactions on Audio, Speech, and Language Processing},
  year     = {2009},
  volume   = {17},
  number   = {6},
  pages    = {1142-1158},
  month    = {Aug},
  issn     = {1558-7916},
  abstract = {The paper considers the task of recognizing environmental sounds for the understanding of a scene or context surrounding an audio sensor. A variety of features have been proposed for audio recognition, including the popular Mel-frequency cepstral coefficients (MFCCs) which describe the audio spectral shape. Environmental sounds, such as chirpings of insects and sounds of rain which are typically noise-like with a broad flat spectrum, may include strong temporal domain signatures. However, only few temporal-domain features have been developed to characterize such diverse audio signals previously. Here, we perform an empirical feature analysis for audio environment characterization and propose to use the matching pursuit (MP) algorithm to obtain effective time-frequency features. The MP-based method utilizes a dictionary of atoms for feature selection, resulting in a flexible, intuitive and physically interpretable set of features. The MP-based feature is adopted to supplement the MFCC features to yield higher recognition accuracy for environmental sounds. Extensive experiments are conducted to demonstrate the effectiveness of these joint features for unstructured environmental sound classification, including listening tests to study human recognition capabilities. Our recognition system has shown to produce comparable performance as human listeners.},
  doi      = {10.1109/TASL.2009.2017438},
  keywords = {audio signal processing;pattern recognition;time-frequency analysis;Mel-frequency cepstral coefficients;audio sensor;broad flat spectrum;human recognition;matching pursuit algorithm;sound classification;sound recognition;temporal domain signatures;time-frequency audio features;Acoustic noise;Acoustic sensors;Cepstral analysis;Chirp;Humans;Insects;Layout;Matching pursuit algorithms;Rain;Spectral shape;Audio classification;Mel-frequency cepstral coefficient (MFCC);auditory scene recognition;data representation;feature extraction;feature selection;matching pursuit},
}

@Article{aucouturier2007bag,
  author    = {Aucouturier, Jean-Julien and Defreville, Boris and Pachet, Francois},
  title     = {The bag-of-frames approach to audio pattern recognition: A sufficient model for urban soundscapes but not for polyphonic music},
  journal   = {The Journal of the Acoustical Society of America},
  year      = {2007},
  volume    = {122},
  number    = {2},
  pages     = {881--891},
  file      = {:JASA-2006.pdf:PDF},
  publisher = {ASA},
}

@Article{1561288,
  author   = {A. J. Eronen and V. T. Peltonen and J. T. Tuomi and A. P. Klapuri and S. Fagerlund and T. Sorsa and G. Lorho and J. Huopaniemi},
  title    = {Audio-based context recognition},
  journal  = {IEEE Transactions on Audio, Speech, and Language Processing},
  year     = {2006},
  volume   = {14},
  number   = {1},
  pages    = {321-329},
  month    = {Jan},
  issn     = {1558-7916},
  abstract = {The aim of this paper is to investigate the feasibility of an audio-based context recognition system. Here, context recognition refers to the automatic classification of the context or an environment around a device. A system is developed and compared to the accuracy of human listeners in the same task. Particular emphasis is placed on the computational complexity of the methods, since the application is of particular interest in resource-constrained portable devices. Simplistic low-dimensional feature vectors are evaluated against more standard spectral features. Using discriminative training, competitive recognition accuracies are achieved with very low-order hidden Markov models (1-3 Gaussian components). Slight improvement in recognition accuracy is observed when linear data-driven feature transformations are applied to mel-cepstral features. The recognition rate of the system as a function of the test sequence length appears to converge only after about 30 to 60 s. Some degree of accuracy can be achieved even with less than 1-s test sequence lengths. The average reaction time of the human listeners was 14 s, i.e., somewhat smaller, but of the same order as that of the system. The average recognition accuracy of the system was 58\% against 69\%, obtained in the listening tests in recognizing between 24 everyday contexts. The accuracies in recognizing six high-level classes were 82\% for the system and 88\% for the subjects.},
  doi      = {10.1109/TSA.2005.854103},
  file     = {:eronen-taslp-2006.pdf:PDF},
  keywords = {audio signal processing;computational complexity;feature extraction;hidden Markov models;pattern classification;14 sec;audio-based context recognition;automatic context classification;computational complexity;discriminative training;feature vectors;hidden Markov models;linear data-driven feature transformations;mel-cepstral features;recognition accuracy;resource-constrained portable devices;spectral features;test sequence length;Acoustic devices;Acoustic signal processing;Computational complexity;Context awareness;Feature extraction;Hidden Markov models;Humans;Mobile handsets;System testing;Vectors;Audio classification;context awareness;feature extraction;hidden Markov models (HMMs)},
}

@Article{clarkson1998auditory,
  author  = {Clarkson, Brian and Sawhney, Nitin and Pentland, Alex},
  title   = {Auditory context awareness via wearable computing},
  journal = {Energy},
  year    = {1998},
  volume  = {400},
  number  = {600},
  pages   = {20},
}

@Article{sawhney1997situational,
  author    = {Sawhney, Nitin and Maes, Pattie},
  title     = {Situational awareness from environmental sounds},
  journal   = {Tech-nical Report, Massachusetts Institute of Technology},
  year      = {1997},
  publisher = {Citeseer},
}

@Article{cauchi2011non,
  author  = {Cauchi, Benjamin},
  title   = {Non-negative matrix factorisation applied to auditory scenes classification},
  journal = {Master's thesis, Master ATIAM, Universit{\'e} Pierre et Marie Curie},
  year    = {2011},
}

@InProceedings{7820783,
  author    = {C. H. Lin and K. M. Cheong and M. C. Huang and M. Y. Chen and C. K. Chang and T. S. Chi},
  title     = {Acoustic scene change detection by spectro-temporal filtering on spectrogram using chirps},
  booktitle = {2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)},
  year      = {2016},
  pages     = {1-5},
  month     = {Dec},
  abstract  = {Indoor acoustic scene change detection systems using periodic impulse signals were developed in the past. Comparing with impulse signals, the chirp signal is more robust against environmental noise due to its specific spectro-temporal structure, which can be easily detected using a spectro-temporal modulation filtering mechanism. In this paper, we demonstrate a system which employs the two-dimensional spectro-temporal filtering mechanism on Fourier spectrogram to measure the total energy of reverberations of the chirp signal and compares the energy difference between consecutive chirps with a predefined threshold to automatically detect the change of the acoustic scene. Simulation results show the proposed system is very effective in detecting the acoustic scene change in a real living room with great robustness against noise recorded from real-world.},
  doi       = {10.1109/APSIPA.2016.7820783},
  file      = {:Acoustic scene change detection by spectro-temporal filtering on spectrogram using chirps.pdf:PDF},
  keywords  = {acoustic signal processing;chirp modulation;filtering theory;reverberation;Fourier spectrogram;chirp signal;chirp signal reverberation energy measurement;indoor acoustic scene change detection system;periodic impulse signal;real living room;two-dimensional spectrotemporal modulation filtering mechanism;Chirp;Filtering;Modulation;Reverberation;Sensors;Spectrogram},
}
@INPROCEEDINGS{7370192, 
author={C. H. Lin and M. Y. Chen and C. K. Chang}, 
booktitle={2015 IEEE SENSORS}, 
title={Acoustic scene change detection}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-4}, 
abstract={This work proposes an acoustic scene change detection approach in the indoor space by reverberation. Speakers emit impulse signals periodicity. Sound waves are then influenced with objects, the floor, walls, and roofs, and generate corresponding reverberation. This reverberation can represent the characteristic of the indoor space, which is called acoustic scene. When the state of the indoor space changes, such as furniture changes positions, windows are open or closed, and the number of people in the room increases or decreases, the acoustic scene would differ from the one before. In this way, we can evaluate whether the indoor scene changes or not. This work presents the difference between the current acoustic scene and the initial acoustic scene by calculating the difference between the current spectrogram and the initial spectrogram of the reverberation. When the sum of acoustic scene differences is greater than the threshold decided in the training stage, we say that acoustic scene change occurs.}, 
keywords={acoustic transducers;reverberation;acoustic scene change detection;current spectrogram;impulse signals periodicity;indoor space;initial spectrogram;reverberation;Microphones;Reflection;Reverberation;Sensors;Spectrogram;Standards;Acoustic scene change detection;acoustic scene modelling;room impulse response;speaker array}, 
doi={10.1109/ICSENS.2015.7370192}, 
ISSN={}, 
month={Nov},}

@InProceedings{7760424,
  author    = {A. Mesaros and T. Heittola and T. Virtanen},
  title     = {TUT database for acoustic scene classification and sound event detection},
  booktitle = {2016 24th European Signal Processing Conference (EUSIPCO)},
  year      = {2016},
  pages     = {1128-1132},
  month     = {Aug},
  abstract  = {We introduce TUT Acoustic Scenes 2016 database for environmental sound research, consisting of binaural recordings from 15 different acoustic environments. A subset of this database, called TUT Sound Events 2016, contains annotations for individual sound events, specifically created for sound event detection. TUT Sound Events 2016 consists of residential area and home environments, and is manually annotated to mark onset, offset and label of sound events. In this paper we present the recording and annotation procedure, the database content, a recommended cross-validation setup and performance of supervised acoustic scene classification system and event detection baseline system using mel frequency cepstral coefficients and Gaussian mixture models. The database is publicly released to provide support for algorithm development and common ground for comparison of different techniques.},
  doi       = {10.1109/EUSIPCO.2016.7760424},
  keywords  = {audio recording;audio signal processing;Gaussian mixture models;TUT database;acoustic scene classification;binaural recordings;environmental sound research;mel frequency cepstral coefficients;sound event detection;Automobiles;Databases;Europe;Event detection;Mel frequency cepstral coefficient;Signal processing},
}

@InProceedings{Kumar:2016:AED:2964284.2964310,
  author    = {Kumar, Anurag and Raj, Bhiksha},
  title     = {Audio Event Detection Using Weakly Labeled Data},
  booktitle = {Proceedings of the 2016 ACM on Multimedia Conference},
  year      = {2016},
  series    = {MM '16},
  pages     = {1038--1047},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {2964310},
  doi       = {10.1145/2964284.2964310},
  file      = {:Audio Event Detection using Weakly Labeled Data.pdf:PDF},
  isbn      = {978-1-4503-3603-1},
  keywords  = {audio content analysis, audio event detection, multiple instance learning, temporal localization},
  location  = {Amsterdam, The Netherlands},
  numpages  = {10},
  url       = {http://doi.acm.org/10.1145/2964284.2964310},
}

@InProceedings{6701847,
  author    = {J. F. Gemmeke and L. Vuegen and P. Karsmakers and B. Vanrumste and H. Van hamme},
  title     = {An exemplar-based NMF approach to audio event detection},
  booktitle = {2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics},
  year      = {2013},
  pages     = {1-4},
  month     = {Oct},
  abstract  = {We present a novel, exemplar-based method for audio event detection based on non-negative matrix factorisation. Building on recent work in noise robust automatic speech recognition, we model events as a linear combination of dictionary atoms, and mixtures as a linear combination of overlapping events. The weights of activated atoms in an observation serve directly as evidence for the underlying event classes. The atoms in the dictionary span multiple frames and are created by extracting all possible fixed-length exemplars from the training data. To combat data scarcity of small training datasets, we propose to artificially augment the amount of training data by linear time warping in the feature domain at multiple rates. The method is evaluated on the Office Live and Office Synthetic datasets released by the AASP Challenge on Detection and Classification of Acoustic Scenes and Events.},
  doi       = {10.1109/WASPAA.2013.6701847},
  file      = {:An exemplar-based NMF approach to audio event detection.pdf:PDF},
  issn      = {1931-1168},
  keywords  = {acoustic signal detection;acoustic signal processing;audio signal processing;matrix decomposition;signal classification;speech recognition;AASP Challenge;Office Live datasets;Office Synthetic datasets;acoustic event classification;acoustic event detection;acoustic scene classification;acoustic scene detection;audio event detection;data scarcity;dictionary atoms;dictionary span multiple frames;exemplar-based NMF approach;linear overlapping event combination;linear time warping;noise robust automatic speech recognition;nonnegative matrix factorisation;possible fixed-length exemplar extraction;Acoustics;Dictionaries;Event detection;Hidden Markov models;Measurement;Noise;Training data;Audio event detection;NMF;exemplars},
}

@InProceedings{6287923,
  author    = {A. Kumar and P. Dighe and R. Singh and S. Chaudhuri and B. Raj},
  title     = {Audio event detection from acoustic unit occurrence patterns},
  booktitle = {2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2012},
  pages     = {489-492},
  month     = {March},
  abstract  = {In most real-world audio recordings, we encounter several types of audio events. In this paper, we develop a technique for detecting signature audio events, that is based on identifying patterns of occurrences of automatically learned atomic units of sound, which we call Acoustic Unit Descriptors or AUDs. Experiments show that the methodology works as well for detection of individual events and their boundaries in complex recordings.},
  doi       = {10.1109/ICASSP.2012.6287923},
  issn      = {1520-6149},
  keywords  = {audio recording;audio signal processing;pattern recognition;AUD;acoustic unit descriptors;acoustic unit occurrence patterns;atomic units;audio event detection;audio recordings;pattern identification;Acoustics;Audio recording;Event detection;Hidden Markov models;Noise;Training;Vectors;AUDs;Acoustic event detection;Acoustic unit descriptors;Audio analysis;Audio retrieval},
}

@InProceedings{4517536,
  author    = {A. Pikrakis and T. Giannakopoulos and S. Theodoridis},
  title     = {Gunshot detection in audio streams from movies by means of dynamic programming and Bayesian networks},
  booktitle = {2008 IEEE International Conference on Acoustics, Speech and Signal Processing},
  year      = {2008},
  pages     = {21-24},
  month     = {March},
  abstract  = {This paper treats gunshot detection in audio streams from movies as a maximization task, where the solution is obtained by means of dynamic programming. The proposed method seeks the sequence of segments and respective class labels, i.e., gunshots vs. all other audio types, that maximize the product of posterior class label probabilities, given the segments' data. The required posterior probabilities are estimated by combining soft classification decisions from a set of Bayesian Network combiners. Tests that have been performed on a large set of audio streams indicate that the proposed method yields high performance in terms of both precision and recall of detected gunshot events.},
  doi       = {10.1109/ICASSP.2008.4517536},
  file      = {:Gunshot detection in audio streams from movies by means of dynamic programming and Bayesian networks.pdf:PDF},
  issn      = {1520-6149},
  keywords  = {audio signal processing;belief networks;dynamic programming;probability;Bayesian networks;audio streams;dynamic programming;gunshot detection;posterior probability;soft classification decision;Bayesian methods;Dynamic programming;Event detection;Gunshot detection systems;Informatics;Motion pictures;Music;Speech;Streaming media;Uniform resource locators;BNs;Dynamic Programming;Gunshot Detection},
}

@InProceedings{4425280,
  author    = {G. Valenzise and L. Gerosa and M. Tagliasacchi and F. Antonacci and A. Sarti},
  title     = {Scream and gunshot detection and localization for audio-surveillance systems},
  booktitle = {2007 IEEE Conference on Advanced Video and Signal Based Surveillance},
  year      = {2007},
  pages     = {21-26},
  month     = {Sept},
  abstract  = {This paper describes an audio-based video surveillance system which automatically detects anomalous audio events in a public square, such as screams or gunshots, and localizes the position of the acoustic source, in such a way that a video-camera is steered consequently. The system employs two parallel GMM classifiers for discriminating screams from noise and gunshots from noise, respectively. Each classifier is trained using different features, chosen from a set of both conventional and innovative audio features. The location of the acoustic source which has produced the sound event is estimated by computing the time difference of arrivals of the signal at a microphone array and using linear-correction least square localization algorithm. Experimental results show that our system can detect events with a precision of 93\% at a false rejection rate of 5\% when the SNR is 10dB, while the source direction can be estimated with a precision of one degree. A real-time implementation of the system is going to be installed in a public square of Milan.},
  doi       = {10.1109/AVSS.2007.4425280},
  file      = {:Scream and Gunshot Detection and Localization for Audio-Surveillance Systems.pdf:PDF},
  keywords  = {acoustic signal detection;audio signal processing;image classification;video surveillance;acoustic source;anomalous audio events;audio features;audio-based video surveillance;gunshot detection;linear-correction localization algorithm;localization;microphone array;scream audiosurveillance systems;scream detection;signal arrival;signal classifier;video-camera;Acoustic arrays;Acoustic noise;Acoustic signal detection;Event detection;Gunshot detection systems;Least squares approximation;Microphone arrays;Real time systems;Time difference of arrival;Video surveillance},
}

@InProceedings{1706829,
  author    = {J. L. Rouas and J. Louradour and S. Ambellouis},
  title     = {Audio Events Detection in Public Transport Vehicle},
  booktitle = {2006 IEEE Intelligent Transportation Systems Conference},
  year      = {2006},
  pages     = {733-738},
  month     = {Sept},
  abstract  = {This paper addresses the problem of automatic audio analysis for aided surveillance application in public transport. The aim of such application is to detect critical situations and to warn the control room. We propose a comparative study of two methods of modelisation/classification of acoustical segments. The problem is quite similar to the 'audio indexing' framework, nevertheless the environment here is very noisy. We present two general frameworks based on Gaussian model mixture (GMM) and support vector machine (SVM) to achieve shout detection in railway embedded environment},
  doi       = {10.1109/ITSC.2006.1706829},
  file      = {:01706829.pdf:PDF},
  issn      = {2153-0009},
  keywords  = {Gaussian processes;acoustic signal processing;audio signal processing;railway engineering;signal classification;support vector machines;surveillance;transportation;Gaussian model mixture;acoustical segment classification;aided surveillance;audio events detection;audio indexing;public transport vehicle;railway embedded environment;support vector machine;Acoustic signal detection;Automatic control;Event detection;Indexing;Rail transportation;Support vector machine classification;Support vector machines;Surveillance;Vehicles;Working environment noise},
}

@InProceedings{1521669,
  author    = {C. Clavel and T. Ehrette and G. Richard},
  title     = {Events Detection for an Audio-Based Surveillance System},
  booktitle = {2005 IEEE International Conference on Multimedia and Expo},
  year      = {2005},
  pages     = {1306-1309},
  month     = {July},
  abstract  = {The present research deals with audio events detection in noisy environments for a multimedia surveillance application. In surveillance or homeland security most of the systems aiming to automatically detect abnormal situations are only based on visual clues while, in some situations, it may be easier to detect a given event using the audio information. This is in particular the case for the class of sounds considered in this paper, sounds produced by gun shots. The automatic shot detection system presented is based on a novelty detection approach which offers a solution to detect abnormality (abnormal audio events) in continuous audio recordings of public places. We specifically focus on the robustness of the detection against variable and adverse conditions and the reduction of the false rejection rate which is particularly important in surveillance applications. In particular, we take advantage of potential similarity between the acoustic signatures of the different types of weapons by building a hierarchical classification system},
  doi       = {10.1109/ICME.2005.1521669},
  issn      = {1945-7871},
  keywords  = {audio acoustics;audio recording;audio signal processing;audio-visual systems;multimedia systems;signal classification;surveillance;telecommunication security;abnormal situation;acoustic signature;adverse condition;audio events detection;audio recording;automatic shot detection system;gun shots;hierarchical classification system;homeland security;multimedia surveillance application;noisy environment;sound production;visual clues;weapons;Acoustic noise;Acoustic signal detection;Audio recording;Event detection;Gunshot detection systems;Multimedia systems;Robustness;Surveillance;Terrorism;Working environment noise},
}

@InProceedings{595320,
  author    = {C. Saraceno and R. Leonardi},
  title     = {Audio as a support to scene change detection and characterization of video sequences},
  booktitle = {1997 IEEE International Conference on Acoustics, Speech, and Signal Processing},
  year      = {1997},
  volume    = {4},
  pages     = {2597-2600 vol.4},
  month     = {Apr},
  abstract  = {A challenging problem to construct video databases is the organization of video information. The development of algorithms able to organize video information according to semantic content of the data is getting more and more important. This will allow algorithms such as indexing and retrieval to work more efficiently. Until now, an attempt to extract semantic information has been performed using only video information. As a video sequence is constructed from a 2-D projection of a 3-D scene, video processing has shown its limitations especially in solving problems such as object identification or object tracking, reducing the ability to extract semantic characteristics. A possibility to overcome the problem is to use additional information. The associated audio signal is then the most natural way to obtain this information. This paper presents a technique which combines video and audio information together for classification and indexing purposes. The classification is performed on the audio signal; a general framework that uses the results of such classification is then proposed for organizing video information},
  doi       = {10.1109/ICASSP.1997.595320},
  file      = {:Audio as a support to scene change detection and characterization of video sequences.pdf:PDF},
  issn      = {1520-6149},
  keywords  = {acoustic signal processing;audio signals;audio systems;image classification;image sequences;indexing;object recognition;video signal processing;visual databases;2-D projection;3-D scene;audio signal;characterization;classification;indexing;object identification;object tracking;organization;retrieval;scene change detection;semantic content;video databases;video sequences;Automation;Cameras;Data mining;Databases;Gunshot detection systems;Indexing;Layout;Organizing;Video compression;Video sequences},
}

@Article{10.2307/40072665,
  author    = {Darrell Conklin and Mathieu Bergeron},
  title     = {Feature Set Patterns in Music},
  journal   = {Computer Music Journal},
  year      = {2008},
  volume    = {32},
  number    = {1},
  pages     = {60--70},
  issn      = {01489267, 15315169},
  file      = {:Feature Set Patterns in Music.pdf:PDF},
  publisher = {The MIT Press},
  url       = {http://www.jstor.org/stable/40072665},
}

@InProceedings{harrison2009implementation,
  author    = {Harrison, Alissa M and Lo, Wai-Kit and Qian, Xiao-jun and Meng, Helen},
  title     = {Implementation of an extended recognition network for mispronunciation detection and diagnosis in computer-assisted pronunciation training},
  booktitle = {International Workshop on Speech and Language Technology in Education},
  year      = {2009},
  file      = {:Implementation of an Extended Recognition Network for Mispronunciation Detection and Diagnosis in Computer-Assisted Pronunciation Training.pdf:PDF},
}

@Article{WITT200095,
  author   = {S.M Witt and S.J Young},
  title    = {Phone-level pronunciation scoring and assessment for interactive language learning},
  journal  = {Speech Communication},
  year     = {2000},
  volume   = {30},
  number   = {2},
  pages    = {95 - 108},
  issn     = {0167-6393},
  abstract = {This paper investigates a method of automatic pronunciation scoring for use in computer-assisted language learning (CALL) systems. The method utilises a likelihood-based `Goodness of Pronunciation' (GOP) measure which is extended to include individual thresholds for each phone based on both averaged native confidence scores and on rejection statistics provided by human judges. Further improvements are obtained by incorporating models of the subject’s native language and by augmenting the recognition networks to include expected pronunciation errors. The various GOP measures are assessed using a specially recorded database of non-native speakers which has been annotated to mark phone-level pronunciation errors. Since pronunciation assessment is highly subjective, a set of four performance measures has been designed, each of them measuring different aspects of how well computer-derived phone-level scores agree with human scores. These performance measures are used to cross-validate the reference annotations and to assess the basic GOP algorithm and its refinements. The experimental results suggest that a likelihood-based pronunciation scoring metric can achieve usable performance, especially after applying the various enhancements.
Zusammenfassung
In diesem Artikel wird eine Methode zur automatischen Bewertung der Aussprache innerhalb eines Systems für computergestütztes Fremdsprachenlernen vorgestellt, welche anhand eines Wahrscheinlichkeitsmaßes, Goodness of Pronunciation (GOP), einen Aussprachewert für jedes Phoneme in einer Äußerung berechnet. Liegt ein solcher Aussprachewert oberhalb eines Schwellwertes, wurde ein Aussprachefehler detektiert. Die Methode wird im folgendem durch individuelle Schwellwerte für jedes Phoneme, durch die Einbindung von Modellen der Muttersprache des Fremdsprachenschülers und durch Erweiterung der Erkennungsnetzwerke mit zu erwartenden Aussprachefehlern verbessert. Die Evaluation der GOP Methode erfolgt mit Hilfe einer speziell für diese Zwecke aufgenommenen Datenbank englischer Sprache mit ausländischen Akzent, die Phonetikern in Bezug auf Aussprachefehler kommentierten. Da Bewertung von Aussprache höchst subjektiv ist, sind vier Meßmethoden zur Evaluation verschiedener Aspekte der Übereinstimmung verschiedener Bewertungen eines Datensatzes entwickelt worden. Die Anwendung dieser Meßmethoden ermöglicht, die Leistung der GOP Methode mit Phonetikern zu vergleichen. Die experimentiellen Ergebnisse deuten darauf hin, daß eine auf Wahrscheinlichkeitsmetrik zur Aussprachebewertung in der Lage ist, in der Praxis anwendbare Ergebnisse zu liefern; dies gilt insbesondere nach der Anwendung der Verbesserungen.},
  doi      = {https://doi.org/10.1016/S0167-6393(99)00044-8},
  file     = {:Phone-level pronunciation scoring and assessment for interactive language learning.pdf:PDF},
  keywords = {Speech recognition, Computer-assisted language learning, Pronunciation assessment, Pronunciation teaching},
  url      = {http://www.sciencedirect.com/science/article/pii/S0167639399000448},
}

@InProceedings{5372889,
  author    = {T. J. Hazen and W. Shen and C. White},
  title     = {Query-by-example spoken term detection using phonetic posteriorgram templates},
  booktitle = {2009 IEEE Workshop on Automatic Speech Recognition Understanding},
  year      = {2009},
  pages     = {421-426},
  month     = {Nov},
  abstract  = {This paper examines a query-by-example approach to spoken term detection in audio files. The approach is designed for low-resource situations in which limited or no in-domain training material is available and accurate word-based speech recognition capability is unavailable. Instead of using word or phone strings as search terms, the user presents the system with audio snippets of desired search terms to act as the queries. Query and test materials are represented using phonetic posteriorgrams obtained from a phonetic recognition system. Query matches in the test data are located using a modified dynamic time warping search between query templates and test utterances. Experiments using this approach are presented using data from the Fisher corpus.},
  doi       = {10.1109/ASRU.2009.5372889},
  file      = {:05372889.pdf:PDF},
  keywords  = {query processing;speech processing;speech recognition;query-by-example spoken term detection;phonetic posteriorgram template;word based speech recognition capability;audio snippets;modified dynamic time warping search;test utterances;Fisher corpus;Speech recognition;System testing;Vocabulary;Acoustic testing;Music information retrieval;Acoustic measurements;Hidden Markov models;Laboratories;Materials testing;Research and development},
}

@InProceedings{gupta2017towards,
  author       = {Gupta, Chitralekha and Grunberg, David and Rao, Preeti and Wang, Ye},
  title        = {Towards automatic mispronunciation detection in singing},
  booktitle    = {Proceedings of the 14th International Society for Music Information Retrieval Conference},
  year         = {2017},
  organization = {ISMIR},
  file         = {:Towards automatic mispronunciation detection in singing.pdf:PDF},
}

@InProceedings{4697411,
  author    = {C. Cao and M. Li and J. Liu and Y. Yan},
  title     = {A study on singing performance evaluation criteria for untrained singers},
  booktitle = {2008 9th International Conference on Signal Processing},
  year      = {2008},
  pages     = {1475-1478},
  month     = {Oct},
  abstract  = {This paper describes a study of subjective criteria for untrained singerspsila singing voice quality evaluation, focusing on the perceptual aspects that have relatively strong acoustic implications. And the correlation among the individual perceptual criteria is also investigated. A SVM regression method is applied to find the importance of every evaluation criterion. Experiments on a 200 singing clips dataset are conducted to give promising results. And probable acoustic cues are introduced for a future prospect.},
  doi       = {10.1109/ICOSP.2008.4697411},
  issn      = {2164-5221},
  keywords  = {audio signal processing;regression analysis;support vector machines;singing performance evaluation criteria;untrained singers;singing voice quality evaluation;SVM regression method;Brightness;Rhythm;Timbre;Dynamic range;Speech analysis;Acoustics;Support vector machines;Humans;Signal processing algorithms;Resonance},
}

@Article{doi:10.1177/0305735614528833,
  author   = {Arla J. Good and Frank A. Russo and Jennifer Sullivan},
  title    = {The efficacy of singing in foreign-language learning},
  journal  = {Psychology of Music},
  year     = {2015},
  volume   = {43},
  number   = {5},
  pages    = {627-640},
  abstract = { This study extends the popular notion that memory for text can be supported by song to foreign-language learning. Singing can be intrinsically motivating, attention focusing, and simply enjoyable for learners of all ages. The melodic and rhythmic context of song enhances recall of native text; however, there is limited evidence that these benefits extend to foreign text. In this study, Spanish-speaking Ecuadorian children learned a novel English passage for 2 weeks. Children in a sung condition learned the passage as a song and children in the spoken condition learned the passage as an oral poem. Children were tested on their ability to recall the passage verbatim, pronounce English vowel sounds, and translate target terms from English to Spanish. As predicted, children in the sung condition outperformed children in the spoken condition in all three domains. The song advantage persevered after a 6-month delay. Findings have important implications for foreign language instruction. },
  doi      = {10.1177/0305735614528833},
  eprint   = {https://doi.org/10.1177/0305735614528833},
  url      = { 
        https://doi.org/10.1177/0305735614528833
    
},
}

@Comment{jabref-meta: databaseType:bibtex;}
